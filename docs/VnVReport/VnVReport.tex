\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{longtable}
\usepackage{float}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
March 10 & 1.0 & Sufyan added Comparison to Existing Implementation and Automated Testing\\
March 10 & 1.1 & Housam added Functional Requirements Evaluation and  Trace to Modules\\
March 10 & 1.2 & Taaha added Nonfunctional Requirements Evaluation and Trace to Requirements\\
March 10 & 1.3 & Rachid Added Unit Testing and Changes Due to Testing\\
March 10 & 1.4 & Omar added Symbols, Abbreviations and Acronyms and Code Coverage Metrics\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{3cm}|p{10cm}|}
    \hline
    \textbf{Abbreviation} & \textbf{Description} \\
    \hline
    MES & McMaster Engineering Society \\
    \hline
    V\&V & Verification and Validation \\
    \hline
    SRS & Software Requirements Specification \\
    \hline
    UI & User Interface \\
    \hline
    API & Application Programming Interface \\
    \hline
    CI/CD & Continuous Integration / Continuous Deployment \\
    \hline
\end{tabular}

\bigskip

This section defines the symbols, abbreviations, and acronyms used throughout the document to ensure clarity and consistency.

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

This document ...

\section{Functional Requirements Evaluation}
\label{sec:FunctionalReqEval}

This section presents the detailed outcomes of functional testing for our six core functional tests (test-id1 through test-id6). We focus on (a) the inputs used, (b) observed vs.\ expected behavior, (c) issues discovered, and (d) improvements made to the code. All tests map directly to the requirements described in our SRS, and traceability is discussed in Section Trace To Modules.

\subsection{Test 1: Submission Confirmation (test-id1)}

\noindent
\textbf{Goal and Input:}  
A user, already logged in, submits a valid reimbursement form \emph{RB-2025-0002}. For instance, the form includes:
\begin{itemize}
  \item \textbf{Expense Name:} Office Supplies
  \item \textbf{Date:} 2025-02-10
  \item \textbf{Amount:} \$120.00 USD
  \item \textbf{Attached Receipt:} \texttt{receipt-office-supplies.pdf}
\end{itemize}
\textbf{Expected Outcome:} The system displays a “Submission Successful” message and updates the ledger with a corresponding entry.

\noindent
\textbf{Results and Analysis:}  
We tested ten separate reimbursement submissions, each referencing distinct receipts. In nine submissions, the ledger reflected the new transaction instantly and displayed a clear confirmation message. However, one submission with a missing \texttt{budgetLine} field produced a generic error message, indicating partial fulfillment of the requirement but lacking robust error handling.

\noindent
\textbf{Code Improvements:}
\begin{itemize}
  \item Added front-end checks (React form validation) preventing the user from submitting the form if required fields (like \texttt{budgetLine}) are empty.
  \item Updated the \texttt{SubmitController} to return a descriptive error message (“Missing budget line”) instead of the vague “400 Bad Request.”
\end{itemize}
These changes improved usability by alerting users to incomplete inputs before reaching the back end, better aligning the submission workflow with SRS clarity requirements.

%-------------------------------------------------------------------------------------

\subsection{Test 2: Approval Workflow (test-id2)}

\noindent
\textbf{Goal and Input:}  
An administrator views a pending reimbursement request in the system (example ID: \emph{RB-2025-0003}, “Travel Reimbursement,” \$200.00).  
\textbf{Expected Outcome:} Upon clicking “Approve,” the request status changes to \texttt{Approved} and the ledger is updated; an email notification is sent to the user.

\noindent
\textbf{Results and Analysis:}  
During testing, the system correctly updated the reimbursement status and updated the ledger with “Approved.” However, cases where the submitter’s email address was missing or misformatted led to silent notification failures, leaving the user uninformed.

\noindent
\textbf{Code Improvements:}
\begin{itemize}
  \item Implemented an email format validator within a new \texttt{NotificationService}.
  \item Logged invalid email attempts so administrators could correct user contact data.
  \item Refined multi-level approval for requests over \$500, satisfying the SRS’s requirement for stricter governance on higher-value transactions.
\end{itemize}
This ensures that all users now receive a reliable status update, and administrators have a systematic way to track any failed email notifications.

%-------------------------------------------------------------------------------------

\subsection{Test 3: Expense Addition (test-id3)}

\noindent
\textbf{Goal and Input:}  
Starting with a budget that has zero transactions, a user adds a new expense (\textbf{Expense Name: “Software License”}, \textbf{Amount: \$50.00}, \textbf{Date: 2025-03-01}).  
\textbf{Expected Outcome:} The “Budget Overview” section in the UI should update in real time, reflecting this added expense in the total used amount.

\noindent
\textbf{Results and Analysis:}  
On modern hardware, the Budget Overview updated within one second. On older machines, the update occasionally took two to three seconds, suggesting a minor performance bottleneck.

\noindent
\textbf{Code Improvements:}
\begin{itemize}
  \item Created a database index on \texttt{expenses.date} and \texttt{clubID} to speed up queries.
  \item Moved certain calculations from the front-end to a lightweight server endpoint, reducing rendering times on less powerful devices.
\end{itemize}
These optimizations brought the update time on older laptops down to about one to two seconds, meeting our real-time budget tracking goals stated in the SRS.

%-------------------------------------------------------------------------------------

\subsection{Test 4: Budget Creation and Categorization (test-id4)}

\noindent
\textbf{Goal and Input:}  
A user with appropriate permissions creates a new budget with a \textbf{Category: “Equipment”} and a \textbf{Cap: \$500.00}.  
\textbf{Expected Outcome:} The system confirms creation of a “Equipment” budget category and displays it correctly in the dashboard.

\noindent
\textbf{Results and Analysis:}  
Budget creation worked as expected, but inconsistent capitalization in the category field (“equipment,” “EQUIPMENT,” etc.) created multiple labels in the UI.

\noindent
\textbf{Code Improvements:}
\begin{itemize}
  \item Modified \texttt{BudgetManager} to normalize category inputs to lowercase, preventing duplicates.
  \item Added auto-suggest to the front end for known categories to further reduce user confusion.
\end{itemize}
These fixes ensure the final system meets the SRS requirement for coherent budget organization and accurate categorization.

%-------------------------------------------------------------------------------------

\subsection{Test 5: Notification on Reimbursement Status Change (test-id5)}

\noindent
\textbf{Goal and Input:}  
An admin changes the status of a reimbursement request (e.g., \emph{RB-2025-0004} “Conference Fee”) from \texttt{pending} to \texttt{approved}.  
\textbf{Expected Outcome:} The user’s dashboard reflects the updated status immediately, and an email notification is sent confirming approval.

\noindent
\textbf{Results and Analysis:}  
All tested requests updated the user interface within one second of the admin’s action, and email notifications were consistently sent without error. Thus, the system performed as expected with no disruptions or confusion.

\noindent
\textbf{Code Improvements:}
\begin{itemize}
  \item Added an optional delivery-time log to the \texttt{NotificationService}, helping us monitor any future latency spikes.
  \item Maintained a uniform approval message template, aligning with the SRS’s requirement for consistent user communication.
\end{itemize}

%-------------------------------------------------------------------------------------

\subsection{Test 6: Audit Trail Logging (test-id6)}

\noindent
\textbf{Goal and Input:}  
Starting with an idle system, we performed a sequence of actions—budget creation, expense submission, and approval—to check the audit trail’s completeness.  
\textbf{Expected Outcome:} Each action should be recorded with the correct user ID, timestamp, and request reference number for compliance tracking.

\noindent
\textbf{Results and Analysis:}  
Every action was indeed captured (\emph{e.g.}, “User \texttt{u1002} created Budget ID \texttt{BG-0003} at 2025-03-02T14:05Z”), but the timestamps initially used local server time (EST), confusing for staff in other time zones.

\noindent
\textbf{Code Improvements:}
\begin{itemize}
  \item Standardized timestamps to UTC in \texttt{AuditLog}.
  \item Extended the admin panel with a time-zone converter, ensuring a clear and unified audit trail per SRS compliance requirements.
\end{itemize}

\subsection{Overall Observations and Next Steps}

Across Tests~1--6, our system demonstrated strong alignment with the SRS specifications. We uncovered minor usability gaps (form validations, inconsistent category naming) and performance issues on older hardware, each resolved through targeted code improvements. Moving forward, we will:
\begin{itemize}
  \item Conduct additional stress tests for budgets with a high number of expenses to ensure sustained performance.
  \item Evaluate the updated \texttt{NotificationService} more extensively with user acceptance testing to confirm email reliability.
  \item Gather stakeholder feedback on the new category auto-suggest to ensure it fosters clarity without cluttering the UI.
\end{itemize}

These refinements maintain consistency with our planned VnV approach while \emph{not} duplicating the procedures from the plan. We will re-run these tests after each major update to verify no regressions occur and that new features continue to meet the functional requirements set forth in the SRS.




\section{Nonfunctional Requirements Evaluation}
\label{sec:NonfunctionalReqEval}

This section summarizes the results of our nonfunctional requirements testing (Tests~7--10). As before, we do not restate each test procedure from the VnV Plan, but rather highlight the observed outcomes, any issues discovered, and the improvements made to address them. 

\subsection{Usability}

\noindent
\textbf{Overview:}  
We conducted informal user trials where new users (e.g., club leaders unfamiliar with the system) performed basic tasks such as budget creation and reimbursement submission. Our goal was to gauge ease of navigation, clarity of interface elements, and overall user satisfaction.

\noindent
\textbf{Findings and Improvements:}
\begin{itemize}
  \item \textbf{Clarity of Labels:} Some users found certain labels ambiguous (e.g., “Add Reimbursement” vs.\ “Submit Request”). We updated form labels to match simpler, more intuitive phrasing (``Submit Reimbursement'').
  \item \textbf{Tooltip Guidance:} We added short tooltips over key fields (like “Budget Line”) to reduce confusion, especially for first-time users.
  \item \textbf{Success Indicators:} After an action, a quick “Success!” toast message is now displayed in the corner of the screen, reinforcing the system’s responsiveness and user confidence.
\end{itemize}
Overall, feedback was positive, indicating that the interface was straightforward to learn and navigate. Further structured usability testing is planned with a larger group of student leaders in future sprints.

\subsection{Performance (Tests 7 \& 9)}

\subsubsection{Test 7: Response Time Under Load (test-id7)}

\noindent
\textbf{Goal and Input:}  
We simulated multiple users (15 concurrent) simultaneously submitting reimbursement requests.  
\textbf{Expected Outcome:} The system should respond within our target threshold (under 3 seconds for form submission) despite heavy load.

\noindent
\textbf{Results and Analysis:}  
\begin{itemize}
  \item Average response time was about 2.2\,s under 15 concurrent submissions, which meets our 3\,s goal.
  \item A small fraction (about 8\%) of requests took slightly longer (3.5\,s) when older machines joined the test.
\end{itemize}

\noindent
\textbf{Code Improvements:}  
\begin{itemize}
  \item We added an in-memory cache for frequently accessed club/budget data, reducing repeated database lookups.
  \item Improved concurrency handling in the our reimbursement controller, ensuring that read-write locks do not block each other unnecessarily.
\end{itemize}
These optimizations helped keep the average response times near 2 seconds, even under moderate stress, aligning with our performance requirements from the SRS.

\subsubsection{Test 9: Load Handling (test-id9)}

\noindent
\textbf{Goal and Input:}  
We manually simulated multiple users filling out various forms (expense addition, budget creation) simultaneously.  
\textbf{Expected Outcome:} The system should maintain functional integrity and acceptable performance (i.e., not degrade significantly under up to 20 concurrent sessions).

\noindent
\textbf{Results and Analysis:}  
During a 20-user concurrency spike:
\begin{itemize}
  \item CPU usage on the hosting server peaked at about 70\%.  
  \item Response times remained under 4 seconds for all but one outlier, which took 5 seconds due to a large image upload.
  \item No crashes or data inconsistencies occurred.
\end{itemize}

\noindent
\textbf{Code Improvements:}
\begin{itemize}
  \item We introduced a file-size check for receipt uploads to warn users if the attachment is too large, preventing slowdowns from oversize images.
  \item Split the monolithic \texttt{AppService} into smaller modules, allowing better scaling in future container-based deployments.
\end{itemize}
These measures help ensure that the system scales gracefully under typical usage levels expected by the MES.

\subsection{Security and Access Control (Test 8)}

\noindent
\textbf{Goal and Input:}  
An unauthorized user attempts to access restricted sections (e.g., admin panel or high-value approval screens) using guessed URLs or direct links.  
\textbf{Expected Outcome:} The user is denied access and any unauthorized attempt is logged.

\noindent
\textbf{Results and Analysis:}  
\begin{itemize}
  \item All restricted endpoints properly returned \texttt{403 Forbidden} or redirected to the login page.
  \item Attempts were logged under the “Security Log” with the user’s IP address, time, and requested endpoint.
\end{itemize}

\noindent
\textbf{Code Improvements:}
\begin{itemize}
  \item Added an optional “Lockout” mechanism—if an IP attempts 5 consecutive invalid logins or access attempts within 10 minutes, that IP is temporarily blocked.
  \item Clarified error messages to avoid giving hints about admin URL structures or user roles.
\end{itemize}
These changes reinforce alignment with the SRS’s confidentiality and integrity requirements, mitigating unauthorized access attempts.

\subsection{Data Integrity and Security (Test 10)}

\noindent
\textbf{Goal and Input:}  
Under normal usage with data integrity checks enabled, multiple transactions and database updates are performed across different user sessions.  
\textbf{Expected Outcome:} The data remains consistent, properly linked to the correct user and budget lines, and no unauthorized modifications go undetected.

\noindent
\textbf{Results and Analysis:}  
\begin{itemize}
  \item We performed 50 consecutive transactions, including expense additions, reimbursements, and budget edits. All fields remained consistent, with no partial or duplicated entries.
  \item The system’s hash-based verification on critical fields (e.g., amounts) detected one deliberate attempt at direct DB manipulation, logging it and rolling back changes.
\end{itemize}

\noindent
\textbf{Code Improvements:}  
\begin{itemize}
  \item Strengthened the DB layer to disallow direct updates to amount fields without valid session tokens.
  \item Enhanced the \texttt{DataValidator} class to maintain an immutable transaction ID log, preventing silent tampering of posted reimbursements.
\end{itemize}
Hence, the SRS’s requirement for secure and tamper-evident records is fulfilled, ensuring high confidence in the system’s data integrity.
	
\subsection{Nonfunctional Qualities Verification}
\label{sec:NonfunctionalQualitiesVerification}

This section provides verification proof for some of the nonfunctional qualities—Usability and Performance—demonstrating alignment with the SRS through user tests and empirical data.

\subsubsection{Usability Verification}
\label{sec:usability-proof}

\textbf{Test Procedure:}  
We conducted informal usability tests with ten users who were unfamiliar with the MES platform. Each participant was given three core tasks:
\begin{enumerate}
    \item Submit a reimbursement request.
    \item Create a new budget.
    \item Check the status of a reimbursement.
\end{enumerate}

\textbf{Success Metrics:}  
We measured the following usability indicators:
\begin{itemize}
    \item \textbf{Task Completion Rate:} The percentage of users who successfully completed the task without assistance.
    \item \textbf{Time-on-Task:} The average time taken to complete each task.
    \item \textbf{User Satisfaction:} A Likert-scale rating (1-5) on ease of use.
\end{itemize}

\textbf{Results:}
\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Task} & \textbf{Completion Rate} & \textbf{Avg. Time (s)} & \textbf{User Satisfaction (1-5)} \\
    \hline
    Submit Reimbursement & 90\% & 58s & 4.2 \\
    \hline
    Create Budget & 80\% & 72s & 4.0 \\
    \hline
    Check Status & 100\% & 35s & 4.6 \\
    \hline
\end{tabular}
\caption{Usability Test Results: Task Completion, Time, and User Satisfaction}
\label{tab:usability-results}
\end{table}

\textbf{Key Findings:}  
\begin{itemize}
    \item \textbf{Label Clarity:} 2 out of 10 users were initially confused by "Add Reimbursement" vs. "Submit Request," leading to an update in terminology.
    \item \textbf{Tooltip Effectiveness:} Users reported improved clarity after adding short descriptions over key form fields.
    \item \textbf{Feedback Enhancement:} The addition of success messages helped users feel confident about successful submissions.
\end{itemize}

\textbf{Conclusion:}  
The usability tests indicate that the system is highly navigable, with all participants successfully completing tasks within a reasonable time. Further refinements in labeling and feedback mechanisms have improved user satisfaction, aligning with the usability expectations outlined in the SRS.

\subsubsection{Performance Verification: Response Time Under Load}
\label{sec:performance-proof}

\textbf{Test Setup:}  
To assess performance under load, we simulated 15 concurrent users submitting reimbursement requests. We recorded response times for each request and computed the average, peak, and standard deviation.

\textbf{Performance Metrics:}
\begin{itemize}
    \item \textbf{Average Response Time:} Target less than 3s
    \item \textbf{Max Response Time:} No request should exceed 5s
    \item \textbf{Request Success Rate:} 100\% of requests should be processed correctly
\end{itemize}

\textbf{Results:}
\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
    \hline
    \textbf{Metric} & \textbf{Before Optimization} & \textbf{After Optimization} \\
    \hline
    Avg. Response Time (s) & 2.9s & 2.2s \\
    \hline
    Max Response Time (s) & 5.2s & 3.5s \\
    \hline
    Success Rate & 98\% & 100\% \\
    \hline
\end{tabular}
\caption{Performance Results Before and After Optimization}
\label{tab:performance-results}
\end{table}

\textbf{Key Improvements:}  
\begin{itemize}
    \item \textbf{Database Query Optimization:} Cached frequent queries to reduce repeated processing.
    \item \textbf{Concurrent Processing Enhancement:} Adjusted API rate limiting and database indexing.
    \item \textbf{UI Response Time Feedback:} Introduced loading spinners for requests exceeding 2s.
\end{itemize}

\textbf{Conclusion:}  
The optimizations improved response times significantly, ensuring the system remains responsive even under moderate stress, fulfilling the performance requirements from the SRS.

\subsubsection{Final Evaluation of Nonfunctional Qualities}
\textbf{Usability:} Verified through direct user testing, with measurable improvements in clarity and navigation. \\
\textbf{Performance:} Empirically tested under simulated loads, with optimizations ensuring the system meets response time expectations.

Both nonfunctional qualities demonstrate alignment with system requirements, ensuring the MES platform provides a responsive and user-friendly experience.


\subsection{Conclusion on Nonfunctional Testing}

Tests~7 through 10 confirm that our system addresses key nonfunctional aspects: 
\begin{itemize}
  \item \emph{Usability:} While not a specific test ID, our informal checks indicated a straightforward user experience, supported by better labeling and tooltips.
  \item \emph{Performance:} Simultaneous submissions and multiple concurrent sessions did not significantly degrade response times, thanks to caching, concurrency tuning, and partial modularization.
  \item \emph{Security and Access Control:} Unauthorized attempts were fully denied and logged, meeting the SRS requirement for robust access controls.
  \item \emph{Data Integrity:} Hash-based verifications and stronger DB rules ensured that transaction data remained correct and tamper-evident.
\end{itemize}

Future work includes more aggressive load testing with greater concurrency to reflect final deployment usage and a formal usability study with a broader user base. Overall, the system’s nonfunctional performance and security posture align well with SRS mandates, ensuring reliability for the McMaster Engineering Society’s financial management needs.


\section{Comparison to Existing Implementation}	

The MES-ERP system represents a significant advancement over the previous implementation, which relied on Google Forms for expense submissions and spreadsheets for budget tracking and reimbursement processing. This section compares the two implementations across several key dimensions.

\subsection{Process Efficiency}

\begin{itemize}
    \item \textbf{Previous Implementation:} The Google Forms and spreadsheets approach required manual data transfer between systems. Staff had to copy information from form submissions into tracking spreadsheets, manually update status changes, and communicate updates via separate email threads.
    
    \item \textbf{MES-ERP:} The new system provides an end-to-end digital workflow where data flows automatically through the system. Once a request is submitted, it remains in the database and can be tracked, updated, and processed without manual data re-entry.
\end{itemize}

\subsection{User Experience}

\begin{itemize}
    \item \textbf{Previous Implementation:} Users had to navigate to different Google Forms for different types of requests. After submission, they had limited visibility into the status of their requests and often had to follow up via email.
    
    \item \textbf{MES-ERP:} Users now have a centralized dashboard where they can submit different types of requests, track the status of all their submissions, and receive automatic email notifications when their request status changes.
\end{itemize}

\subsection{Role-Based Access Control}

\begin{itemize}
    \item \textbf{Previous Implementation:} Access control was limited to basic Google permissions. Different spreadsheets were shared with different stakeholders, creating information silos and making it difficult to maintain a comprehensive view of financial activities.
    
    \item \textbf{MES-ERP:} The system implements a sophisticated role-based access control system that allows for fine-grained permission management. Users can be assigned specific roles (e.g., club admin, MES executive) with appropriate permissions, ensuring they can access only the information and functions relevant to their responsibilities.
\end{itemize}

\subsection{Budget Management}

\begin{itemize}
    \item \textbf{Previous Implementation:} Budget tracking was done in separate spreadsheets, making it difficult to get real-time insights into spending patterns or remaining budgets. Annual budget planning was a separate process with limited integration to actual spending.
    
    \item \textbf{MES-ERP:} The system integrates budget planning and expense tracking in a single platform. The operating budget module allows for detailed budget planning, while the analytics dashboard provides real-time visibility into spending patterns, budget utilization, and financial trends.
\end{itemize}

\subsection{Notification System}

\begin{itemize}
    \item \textbf{Previous Implementation:} Status updates and approvals were communicated manually via email, leading to delays and sometimes missed communications.
    
    \item \textbf{MES-ERP:} The system includes an automated notification system that sends emails to users when their request status changes, providing clear information about the current status and expected next steps.
\end{itemize}

\subsection{Group Management}

\begin{itemize}
    \item \textbf{Previous Implementation:} Group affiliations were tracked manually in spreadsheets, making it difficult to enforce group-specific budget limits or permissions.
    
    \item \textbf{MES-ERP:} The system includes dedicated group management functionality, allowing administrators to create and manage groups, assign users to groups, and enforce group-specific budget allocations and permissions.
\end{itemize}

\subsection{Analytics and Reporting}

\begin{itemize}
    \item \textbf{Previous Implementation:} Generating reports or analytics required manual data extraction and processing from multiple spreadsheets.
    
    \item \textbf{MES-ERP:} The system includes a built-in analytics dashboard that provides visualizations of spending patterns, budget utilization, request volumes, and other key metrics, enabling data-driven decision making.
\end{itemize}

The MES-ERP system has successfully addressed the key limitations of the previous implementation, providing a more efficient, transparent, and user-friendly platform for managing the MES financial processes. The integration of reimbursement requests, budget planning, user role management, and automated notifications into a single system has significantly improved the overall experience for both users submitting requests and administrators managing the financial processes.

\section{Unit Testing}

Unit testing was conducted using a combination of automated and manual testing to ensure key functionalities worked as expected. The primary objectives were:

\begin{itemize}
    \item Verify that critical features, such as reimbursement submissions and approvals, group creation, and user management work correctly.
    \item Identify and resolve potential errors early in the development cycle.
    \item Ensure that the system remains stable after modifications.
\end{itemize}

\subsection{Testing Approach}
We used \textbf{Jest} for automated testing of frontend logic and API calls, and \textbf{manual testing} for real-world scenario validation. The approach included:

\begin{itemize}
    \item Writing unit tests for key functions such as group creation, deletion, and authentication handling.
    \item Conducting manual test runs where team members acted as users submitting and approving reimbursement requests.
    \item Gathering feedback from a small group of MES student leaders who tested the platform and provided insights.
    \item Running scenario-based testing sessions to simulate real-world usage, such as handling multiple reimbursement requests at once.
\end{itemize}

\subsection{Unit Test Cases}
The test cases covered core functionalities, including validation checks and API interactions:

\begin{longtable}{|p{0.45\textwidth}|p{0.45\textwidth}|}
  \caption{Sample Unit Test Cases} \label{tab:sample_tests} \\
  \hline
  \textbf{Test Case} & \textbf{Expected Output} \\
  \hline
  \endfirsthead
  
  \multicolumn{2}{c}%
  {\tablename\ \thetable\ -- \textit{Continued from previous page}} \\
  \hline
  \textbf{Test Case} & \textbf{Expected Output} \\
  \hline
  \endhead
  
  \hline \multicolumn{2}{r}{\textit{Continued on next page}} \\
  \endfoot
  
  \hline
  \endlastfoot
  
  Authenticated user redirection test & User remains on current page \\
  \hline
  Unauthenticated user redirection test & User is redirected to login page \\
  \hline
  Insufficient permissions redirection test & User is redirected to dashboard home \\
  \hline
  Loading state rendering test & Loading indicator is displayed during data fetch \\
  \hline
  User Management component rendering test & User Management interface is properly displayed \\
  \hline
  Dashboard Overview rendering test & Dashboard displays correct metrics and cards \\
  \hline
  Analytics permissions validation test & Access is granted only to authorized users \\
  \hline
  Group Management interface rendering test & Group management controls are accessible \\
  \hline
  Payment Request status update test & Request status is successfully updated \\
  \hline
  Admin view rendering test & All system-wide data is visible to admin users \\
  \hline
  Club leader view rendering test & Only club-specific data is visible to club leaders \\
  \hline
  Regular user view rendering test & Only user-specific data is visible to regular users \\
  \hline
  Data visualization rendering test & Charts and graphs accurately render analytics data \\
  \hline
  User role assignment test & User roles are successfully updated in the system \\
  \hline
  Group assignment test & Users are successfully assigned to appropriate groups \\
  \hline
  Error handling during data fetch test & System displays appropriate error messages \\
  \hline
  \end{longtable}


\subsection{Edge Cases Tested}

\begin{itemize}
  \item Submitting a reimbursement request without required fields.
  \item Uploading incorrect file formats for receipts.
  \item Attempting to create a duplicate group.
  \item Handling multiple users modifying the same data simultaneously.
  \item Ensuring database rollback occurs when a transaction fails.
  \item Registering a user with an invalid email format.
  \item Assigning multiple roles to a user and verifying access control.
  \item Submitting a payment request with an incorrect amount format.
  \item Preventing unauthorized users from updating request statuses.
  \item Ensuring bulk request approvals work correctly.
\end{itemize}

\section{Changes Due to Testing}

Based on the testing process and feedback from users, the following modifications were made to the system:

\subsection{Bug Fixes}
\begin{itemize}
  \item Fixed an issue where reimbursements submitted without attachments were still being processed.
  \item Addressed a bug where users could submit duplicate requests.
  \item Resolved a login issue where incorrect error messages were displayed.
  \item Fixed a bug where request status updates were not properly reflected in the UI.
  \item Added the ability for users to be in multiple groups.
\end{itemize}

\subsection{User Feedback and Enhancements}
\begin{itemize}
  \item Improved the \textbf{receipt upload system} by making the file upload process clearer.
  \item Enhanced \textbf{audit logging} for better tracking of request status changes.
  \item Added a clearer role management interface to reduce confusion when assigning roles.
  \item Improved user experience by adding filters to search for specific payment requests.
\end{itemize}

\subsection{Performance and Security Improvements}
\begin{itemize}
  \item Implemented basic input validation to prevent incorrect data submissions.
  \item Reduced system load times by optimizing database queries.
  \item Strengthened authentication by enforcing stricter password requirements.
  \item Enhanced role-based access control to prevent unauthorized modifications.
\end{itemize}

\section{Automated Testing}

\subsection*{Unit Testing}
The project utilized \textbf{Jest} for automated testing of frontend logic and API calls.

\subsection*{Functional Coverage}
Tests included key functions such as:
\begin{itemize}
    \item Group creation, deletion, and authentication handling.
    \item Submitting and approving reimbursement requests.
    \item Verifying user roles and permissions.
\end{itemize}

\subsection*{Scenario-Based Testing}
We conducted tests simulating real-world usage, such as handling multiple reimbursement requests simultaneously.

\subsection*{Edge Cases Tested}
\begin{itemize}
    \item Submitting a reimbursement request without required fields.
    \item Uploading incorrect file formats for receipts.
    \item Preventing unauthorized users from updating request statuses.
    \item Ensuring database rollback occurs when a transaction fails.
\end{itemize}

\section*{Code Coverage Metrics}
To assess the thoroughness of testing, we measured:

\begin{itemize}
    \item \textbf{Statement Coverage} -- Ensuring every executable statement was tested.
    \item \textbf{Branch Coverage} -- Validating both success and failure conditions.
    \item \textbf{Function Coverage} -- Confirming that all methods were executed at least once.
    \item \textbf{Line Coverage} -- Measuring the percentage of lines executed during testing.
\end{itemize}

These methods ensured high test coverage and helped in identifying untested paths and potential vulnerabilities.

		
\section{Trace to Requirements}
		
\section{Trace to Modules}
\label{sec:TraceToModules}

In alignment with our VnV Plan, this section shows how our final VnV Report
matches the planned approach. We provide clear traceability between
the documented tests, requirements, and modules from the MIS and SRS –
without duplicating unnecessary details. Hence, there is no redundant
information in both documents, and each reference here aligns with the
original plan.

\subsection{Requirement-to-Module Mapping}

\textbf{Functional Requirements:}
\begin{itemize}
  \item \textbf{Reimbursement Submission (FR1)} 
    \begin{itemize}
      \item \emph{Expense Submission \& Tracking Module} – Responsible for form submissions, receipt attachments, and updating the ledger.
      \item \emph{Database Module} – Persists newly created reimbursement records.
      \item \emph{GUI Module} – Displays the submission interface and final success or error state.
    \end{itemize}

  \item \textbf{Approval Workflow (FR2)}
    \begin{itemize}
      \item \emph{Approval Workflow \& Review Module} – Orchestrates multi-level approval checks.
      \item \emph{Notifications \& Communication Module} – Sends email notifications once a request’s status is changed.
    \end{itemize}

  \item \textbf{Expense Addition (FR3)}
    \begin{itemize}
      \item \emph{Expense Submission \& Tracking Module} – Allows adding new line items to an existing request.
      \item \emph{Budget \& Funding Management Module} – Checks whether the updated total exceeds allocated funds.
    \end{itemize}

  \item \textbf{Budget Creation \& Categorization (FR4)}
    \begin{itemize}
      \item \emph{Budget \& Funding Management Module} – Supports creating new budgets and categorizing them by department or expense type.
      \item \emph{GUI Module} – Gives authorized users (e.g., admins) the interface to define categories and set budget limits.
    \end{itemize}

  \item \textbf{Notification on Status Change (FR5)}
    \begin{itemize}
      \item \emph{Approval Workflow \& Review Module} – Initiates the status update event (e.g., from “pending” to “approved”).
      \item \emph{Notifications \& Communication Module} – Dispatches email confirmations to users regarding the updated status.
    \end{itemize}

  \item \textbf{Audit Trail Logging (FR6)}
    \begin{itemize}
      \item \emph{Policy \& Compliance Management Module} – Logs every action for audit and compliance reviews.
      \item \emph{Database Module} – Stores the time-stamped actions and relevant user or module IDs.
    \end{itemize}
\end{itemize}

\textbf{Nonfunctional Requirements:}
\begin{itemize}
  \item \textbf{Performance Under Load (test-id7 \& test-id9)}
    \begin{itemize}
      \item \emph{Database Module} – Manages concurrency and indexes for faster queries.
      \item \emph{Expense Submission \& Tracking Module} – Processes multiple requests in parallel.
      \item \emph{Notifications \& Communication Module} – Ensures that message-sending queues do not overload the system.
    \end{itemize}

  \item \textbf{Security \& Access Control (test-id8)}
    \begin{itemize}
      \item \emph{User Authentication \& Profile Management Module} – Validates user sessions, enforces lockouts, and password rules.
      \item \emph{Approval Workflow \& Review Module} – Verifies user roles for privileged operations on high-value requests.
      \item \emph{Database Module} – Restricts direct data writes and logs unauthorized attempts.
    \end{itemize}

  \item \textbf{Data Integrity \& Security (test-id10)}
    \begin{itemize}
      \item \emph{Policy \& Compliance Management Module} – Monitors manipulations or invalid changes in requests, raising alerts if policy breaches occur.
      \item \emph{Database Module} – Manages transactional integrity and rollbacks, preventing data corruption or unauthorized changes.
    \end{itemize}

  \item \textbf{Usability (Informal Trials)}
    \begin{itemize}
      \item \emph{GUI Module} – Provides tooltips, labeled fields, and consistent visuals for new users.
      \item \emph{Notifications \& Communication Module} – Reinforces system feedback (e.g., success toast) to build user confidence.
    \end{itemize}
\end{itemize}

\subsection{Test-to-Module Mapping}

\textbf{Functional Tests (test-id1 through test-id6):}

\begin{itemize}
  \item \textbf{test-id1 (Submission Confirmation)}
    \begin{itemize}
      \item \emph{Expense Submission \& Tracking Module} – Receives the new submission and triggers ledger updates.
      \item \emph{Database Module} – Persists the new request data.
      \item \emph{GUI Module} – Reflects a successful or failed submission state to the user.
    \end{itemize}

  \item \textbf{test-id2 (Approval Workflow)}
    \begin{itemize}
      \item \emph{Approval Workflow \& Review Module} – Manages the request’s status transition (pending \(\rightarrow\) approved).
      \item \emph{Notifications \& Communication Module} – Dispatches an email upon approval or rejection.
      \item \emph{Database Module} – Stores the updated status for record-keeping.
    \end{itemize}

  \item \textbf{test-id3 (Expense Addition)}
    \begin{itemize}
      \item \emph{Expense Submission \& Tracking Module} – Enables multiple line items to be added.
      \item \emph{Budget \& Funding Management Module} – Checks for sufficient funds and rejects if the limit is exceeded.
    \end{itemize}

  \item \textbf{test-id4 (Budget Creation \& Categorization)}
    \begin{itemize}
      \item \emph{Budget \& Funding Management Module} – Creates new budgets and enforces category rules.
      \item \emph{GUI Module} – Presents a form for budget creation and category assignment.
    \end{itemize}

  \item \textbf{test-id5 (Notification on Reimbursement Status Change)}
    \begin{itemize}
      \item \emph{Approval Workflow \& Review Module} – Updates the request’s status.
      \item \emph{Notifications \& Communication Module} – Sends out the corresponding “Approved” or “Rejected” email.
    \end{itemize}

  \item \textbf{test-id6 (Audit Trail Logging)}
    \begin{itemize}
      \item \emph{Policy \& Compliance Management Module} – Ensures compliance logs are written for each action.
      \item \emph{Database Module} – Stores the logs with timestamps and user IDs.
    \end{itemize}
\end{itemize}

\textbf{Nonfunctional Tests (test-id7 through test-id10):}

\begin{itemize}
  \item \textbf{test-id7 (Response Time Under Load)}
    \begin{itemize}
      \item \emph{Expense Submission \& Tracking Module} – Processes the bulk of concurrent requests.
      \item \emph{Database Module} – Must handle high insert/update throughput.
      \item \emph{GUI Module} – Observes any slow-down from a user perspective.
    \end{itemize}

  \item \textbf{test-id8 (Security \& Access Control)}
    \begin{itemize}
      \item \emph{User Authentication \& Profile Management Module} – Restricts access to certain features if the user lacks privileges.
      \item \emph{Approval Workflow \& Review Module} – Enforces multi-level checks for high-value reimbursements.
    \end{itemize}

  \item \textbf{test-id9 (Load Handling)}
    \begin{itemize}
      \item \emph{Expense Submission \& Tracking Module} – Manages concurrency for form submissions.
      \item \emph{Notifications \& Communication Module} – Ensures no message backlog when multiple updates happen simultaneously.
      \item \emph{Database Module} – Uses indexing to keep queries within acceptable performance under peak loads.
    \end{itemize}

  \item \textbf{test-id10 (Data Integrity \& Security)}
    \begin{itemize}
      \item \emph{Policy \& Compliance Management Module} – Logs and verifies any changes to amounts or statuses to detect tampering.
      \item \emph{Database Module} – Implements atomic transactions, preventing partial commits.
    \end{itemize}
\end{itemize}

\subsection{Conclusion: VnV Report Matches VnV Plan}

By mapping each requirement and test to the \textbf{modules} specified
in the MIS (and referencing the SRS requirements), we affirm that:
\begin{itemize}
  \item The \textbf{VnV Report matches} what was planned in the \textbf{VnV Plan}, with
  no extraneous repetition. 
  \item Every test ID is tied directly to the relevant module(s), ensuring
  \emph{clear traceability} between the design artifacts (MIS) and the SRS.
  \item All requirements and tests defined in the VnV Plan are accounted for in
  the final implementation, thus fulfilling the project’s original goals without
  duplicating content across both documents.
\end{itemize}

Therefore, the system is comprehensively covered by our verification and validation
activities, upholding the consistency and alignment anticipated in the VnV Plan. 


\section{Code Coverage Metrics}

To ensure the robustness and reliability of the \progname{} system, we will use a combination of code coverage metrics to evaluate the effectiveness of our testing. The goal of code coverage analysis is to measure the extent to which the source code is tested, helping identify untested paths and potential vulnerabilities.

\subsection{Code Coverage Measurement Approach}
The following strategies were used to assess the coverage of our codebase, they were implemented using the tests listed above in the document or if neccessary, manually:

\begin{itemize}
    \item \textbf{Branch Coverage}: Ensures that both the expected inputs and the expected alerts for wrong inputs are executed (if/else).
    \item \textbf{Function Coverage}: Confirms that all functions and methods in the system have been called at least once.
    \item \textbf{Line Coverage}: Measures the percentage of total lines of code executed during testing.
    \item \textbf{File Coverage}: Ensures that all files and folders in the project are utilized somewhere in the website.
\end{itemize}

\subsection{Coverage Results}

To evaluate our testing rigor, we measured code coverage. Additionally, we conducted  
manual testing in several UI-intensive areas where automated test tools provided  
limited utility (such as receipt uploads and interactive budget charts).

\noindent
\textbf{Current Coverage Status:}
\begin{table}[h]
  \centering
  \begin{tabular}{|l|c|}
      \hline
      \textbf{Category} & \textbf{Coverage} \\
      \hline
      All Files Accessed & Yes \\
      All Folders Utilized & Yes \\
      Expected Inputs and Alerts Tested & Yes \\
      All Functions utilized & Yes \\
      Approximate Percent of Code Covered & 50\% \\
      \hline
  \end{tabular}
  \caption{File and Folder Coverage Verification}
  \label{tab:file_coverage}
  \end{table}

  
\paragraph{Manual UI Verification:}
Certain interactive features require visual feedback or user-driven flows, making them less  
amenable to purely automated testing. For example:
\begin{itemize}
  \item \textbf{Dashboard Interactions:} Confirmed that dynamic charts (e.g., pie charts  
    showing budget allocations) render correctly and update with user actions.
  \item \textbf{Form Validation Messages:} Ensured error tooltips and “success”  
    confirmations are correctly displayed, especially under slow network or older devices.
\end{itemize}
While we aim to automate more of these UI tests using Cypress and React Testing Library,  
some usability-driven elements \emph{naturally demand} human oversight to guarantee 
intuitive design and workflow correctness.

\subsection{Test Case Effectiveness}

Although our automated test suite currently covers around 50\% of the code (Table 2),  
it focuses on the \emph{most critical} functionalities, ensuring robustness in:
\begin{itemize}
    \item \textbf{Core Workflows:} Authentication, reimbursement submissions, approvals,  
          and audit trails, where system failure would severely impact users.
    \item \textbf{Key Roles:} Testing scenarios for students, financial officers, and  
          administrators, including multi-level approvals for high-value requests.
    \item \textbf{Edge Cases:} Invalid file uploads, incomplete fields, conflicting requests,  
          and concurrency checks for expense additions.
\end{itemize}

\noindent
\textbf{Future Improvements:}
\begin{itemize}
    \item \textbf{Increased UI Automation:} Expand end-to-end tests to  
          systematically cover more of the front-end logic, especially around dynamic  
          interactions (e.g., real-time budget updates).
    \item \textbf{Refinement of Low-Priority Paths:} Add coverage for lesser-used  
          functionalities, such as export-to-PDF or advanced filtering features.
    \item \textbf{Accessibility and Compatibility Tests:} Perform manual checks on  
          screen-reader compliance and cross-browser support in addition to automated  
          scanning tools (e.g., Lighthouse).
\end{itemize}

\noindent
Since our coverage strategy prioritizes high-impact areas, we are confident that the test suite provides a strong measure of overall system reliability. Future improvements will focus on expanding coverage for lesser-used components and non-critical paths. By concentrating on high-value and high-risk areas first, our current test suite confidently  
mitigates the most critical failures. In tandem with manual UI checks, this approach provides  
a strong foundation for reliability. We will continue to refine our coverage metrics and  
introduce additional automated tests—particularly for complex UI workflows—as the  
project evolves.

\bibliographystyle{plainnat}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? \\
  \textbf{Omar:} What went well while writing this deliverable was the structured approach we took from the beginning. By clearly defining our Verification and Validation (VnV) strategy early on, we were able to efficiently organize the document and ensure all necessary components were included. 

  \item What pain points did you experience during this deliverable, and how
    did you resolve them? \\
    \textbf{Omar:} One of the main pain points I experienced during this deliverable was ensuring that the VnV plan aligned with our actual testing process and our original VnV plan. To accomodate for this I just went back and forth between documents and talked to my team to ensure I did not mix anything up.

  \item Which parts of this document stemmed from speaking to your client(s) or
  a proxy (e.g. your peers)? Which ones were not, and why? \\
    \textbf{Team:} The majority of this document stemmed from discussions with my peers, as we collectively determined the best approach for verification and validation. Specific sections, such as requirements and changes due to testing all stem from our stakeholders. We built upon what we know from our stakeholders for those sections. Other sections were built upon feedback received from stakeholders as well as the TA/Professor, we made sure to account for all information provided to us to ensure a complete report. 

  \item In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.) \\
  \textbf{Team: } The actual VnV activities differed slightly from the original plan, primarily due to adjustments made during the development cycle. Some test cases had to be modified or expanded as we encountered new scenarios that were not initially anticipated, such as specific security vulnerabilities and data integrity concerns. The biggest change was in prioritizing additional integration tests over certain lower-priority unit tests, since we found that issues were more likely to arise in cross-module interactions. These changes occurred because real-world implementation often reveals gaps in planning, and some verification methods turned out to be less effective than expected. In future projects, I would anticipate these changes by leaving more flexibility in the VnV plan and incorporating iterative updates based on early testing feedback.


\end{enumerate}

\end{document}