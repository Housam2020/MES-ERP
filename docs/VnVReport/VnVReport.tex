\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{longtable}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can reference the SRS tables if needed}

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

This document ...

\section{Functional Requirements Evaluation}

\section{Nonfunctional Requirements Evaluation}

\subsection{Usability}
		
\subsection{Performance}

\subsection{etc.}
	
\section{Comparison to Existing Implementation}	

This section will not be appropriate for every project.

\section{Unit Testing}

Unit testing was conducted using a combination of automated and manual testing to ensure key functionalities worked as expected. The primary objectives were:

\begin{itemize}
    \item Verify that critical features, such as reimbursement submissions and approvals, group creation, and user management work correctly.
    \item Identify and resolve potential errors early in the development cycle.
    \item Ensure that the system remains stable after modifications.
\end{itemize}

\subsection{Testing Approach}
The team used \textbf{Jest} for automated testing of frontend logic and API calls, and \textbf{manual testing} for real-world scenario validation. The approach included:

\begin{itemize}
    \item Writing unit tests for key functions such as group creation, deletion, and authentication handling.
    \item Conducting manual test runs where team members acted as users submitting and approving reimbursement requests.
    \item Gathering feedback from a small group of MES student leaders who tested the platform and provided insights.
    \item Running scenario-based testing sessions to simulate real-world usage, such as handling multiple reimbursement requests at once.
\end{itemize}

\subsection{Unit Test Cases}
The test cases covered core functionalities, including validation checks and API interactions:

\begin{longtable}{|p{0.45\textwidth}|p{0.45\textwidth}|}
  \caption{Sample Unit Test Cases} \label{tab:sample_tests} \\
  \hline
  \textbf{Test Case} & \textbf{Expected Output} \\
  \hline
  \endfirsthead
  
  \multicolumn{2}{c}%
  {\tablename\ \thetable\ -- \textit{Continued from previous page}} \\
  \hline
  \textbf{Test Case} & \textbf{Expected Output} \\
  \hline
  \endhead
  
  \hline \multicolumn{2}{r}{\textit{Continued on next page}} \\
  \endfoot
  
  \hline
  \endlastfoot
  
  Create a group with a unique name & Group is successfully created \\
  \hline
  Attempt to create a group with an existing name & System displays an error message \\
  \hline
  Unauthorized user attempts to access group management & Access is denied with error message \\
  \hline
  Delete a group with no assigned users & Group is successfully deleted \\
  \hline
  Attempt to delete a group with assigned users & System prevents deletion and notifies the user \\
  \hline
  Handle failed database connection during group fetch & Error is logged, and fallback UI is displayed \\
  \hline
  Register a new user with valid credentials & User is successfully created and logged in \\
  \hline
  Attempt to register with an existing email & System displays an error message \\
  \hline
  Login with correct credentials & User is authenticated and redirected to the dashboard \\
  \hline
  Login with incorrect credentials & System displays an error message \\
  \hline
  Assign a role to a user & User is successfully assigned the role \\
  \hline
  Remove a role from a user & User no longer has the role permissions \\
  \hline
  Attempt to assign an invalid role & System displays an error message \\
  \hline
  Submit a payment request with valid details & Request is successfully submitted \\
  \hline
  Submit a payment request missing required fields & System displays an error message \\
  \hline
  View all payment requests as an admin & Admin sees all requests \\
  \hline
  View only personal payment requests as a user & User sees only their own requests \\
  \hline
  Attempt to approve a request without proper permissions & Access is denied with an error message \\
  \hline
  Update request status successfully & Request status is updated \\
  \hline
  Handle failed database connection when fetching requests & Error is logged, and fallback UI is displayed \\
  \hline
  
  \end{longtable}


\subsection{Edge Cases Tested}

\begin{itemize}
  \item Submitting a reimbursement request without required fields.
  \item Uploading incorrect file formats for receipts.
  \item Attempting to create a duplicate group.
  \item Handling multiple users modifying the same data simultaneously.
  \item Ensuring database rollback occurs when a transaction fails.
  \item Registering a user with an invalid email format.
  \item Assigning multiple roles to a user and verifying access control.
  \item Submitting a payment request with an incorrect amount format.
  \item Preventing unauthorized users from updating request statuses.
  \item Ensuring bulk request approvals work correctly.
\end{itemize}

\section{Changes Due to Testing}

Based on the testing process and feedback from users, the following modifications were made to the system:

\subsection{Bug Fixes}
\begin{itemize}
  \item Fixed an issue where reimbursements submitted without attachments were still being processed.
  \item Addressed a bug where users could submit duplicate requests.
  \item Resolved a login issue where incorrect error messages were displayed.
  \item Fixed a bug where request status updates were not properly reflected in the UI.
  \item Added the ability for users to be in multiple groups.
\end{itemize}

\subsection{User Feedback and Enhancements}
\begin{itemize}
  \item Improved the \textbf{receipt upload system} by making the file upload process clearer.
  \item Enhanced \textbf{audit logging} for better tracking of request status changes.
  \item Added a clearer role management interface to reduce confusion when assigning roles.
  \item Improved user experience by adding filters to search for specific payment requests.
\end{itemize}

\subsection{Performance and Security Improvements}
\begin{itemize}
  \item Implemented basic input validation to prevent incorrect data submissions.
  \item Reduced system load times by optimizing database queries.
  \item Strengthened authentication by enforcing stricter password requirements.
  \item Enhanced role-based access control to prevent unauthorized modifications.
\end{itemize}

\section{Automated Testing}
		
\section{Trace to Requirements}
		
\section{Trace to Modules}		

\section{Code Coverage Metrics}

To ensure the robustness and reliability of the \progname{} system, we will use a combination of code coverage metrics to evaluate the effectiveness of our testing. The goal of code coverage analysis is to measure the extent to which the source code is tested, helping identify untested paths and potential vulnerabilities.

\subsection{Code Coverage Measurement Approach}
The following strategies were used to assess the coverage of our codebase:

\begin{itemize}
    \item \textbf{Statement Coverage}: Verifies that each executable statement in the code has been executed at least once.
    \item \textbf{Branch Coverage}: Ensures that both the expected inputs and the expected alerts for wrong inputs are executed (if/else).
    \item \textbf{Function Coverage}: Confirms that all functions and methods in the system have been called at least once.
    \item \textbf{Line Coverage}: Measures the percentage of total lines of code executed during testing.
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{../../refs/References}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? \\
  \textbf{Omar:} What went well while writing this deliverable was the structured approach we took from the beginning. By clearly defining our Verification and Validation (VnV) strategy early on, we were able to efficiently organize the document and ensure all necessary components were included. 

  \item What pain points did you experience during this deliverable, and how
    did you resolve them? \\
    \textbf{Omar:} One of the main pain points I experienced during this deliverable was ensuring that the VnV plan aligned with our actual testing process and our original VnV plan. To accomodate for this I just went back and forth between documents and talked to my team to ensure I did not mix anything up.

  \item Which parts of this document stemmed from speaking to your client(s) or
  a proxy (e.g. your peers)? Which ones were not, and why? \\
    \textbf{Team:} The majority of this document stemmed from discussions with my peers, as we collectively determined the best approach for verification and validation. Specific sections, such as requirements and changes due to testing all stem from our stakeholders. We built upon what we know from our stakeholders for those sections. Other sections were built upon feedback received from stakeholders as well as the TA/Professor, we made sure to account for all information provided to us to ensure a complete report. 

  \item In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.) \\
  \textbf{Team: } The actual VnV activities differed slightly from the original plan, primarily due to adjustments made during the development cycle. Some test cases had to be modified or expanded as we encountered new scenarios that were not initially anticipated, such as specific security vulnerabilities and data integrity concerns. The biggest change was in prioritizing additional integration tests over certain lower-priority unit tests, since we found that issues were more likely to arise in cross-module interactions. These changes occurred because real-world implementation often reveals gaps in planning, and some verification methods turned out to be less effective than expected. In future projects, I would anticipate these changes by leaving more flexibility in the VnV plan and incorporating iterative updates based on early testing feedback.


\end{enumerate}

\end{document}