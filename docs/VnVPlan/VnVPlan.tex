\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Nov 4, 2024 & 1.1 & Housam: Add parts to section 3 (plan)\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
N/A

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{3cm}|p{10cm}|}
    \hline
    \textbf{Abbreviation} & \textbf{Description} \\
    \hline
    MES & McMaster Engineering Society \\
    \hline
    V\&V & Verification and Validation \\
    \hline
    SRS & Software Requirements Specification \\
    \hline
    UI & User Interface \\
    \hline
    API & Application Programming Interface \\
    \hline
    CI/CD & Continuous Integration / Continuous Deployment \\
    \hline
\end{tabular}

\bigskip

This section defines the symbols, abbreviations, and acronyms used throughout the document to ensure clarity and consistency.


\newpage

\pagenumbering{arabic}

This document outlines the Verification and Validation (V\&V) Plan for the McMaster Engineering Society Custom Financial Expense Reporting Platform. The V\&V plan will ensure that the software performs as expected, meets user requirements, is secure, and reliable. The primary goals of this document are to define the objectives, methods, and scope of the V\&V process, covering aspects such as requirements validation, design verification, and system testing.

\section{General Information}

\subsection{Summary}

The MES Custom Financial Expense Reporting Platform is a web-based application designed to streamline financial operations for McMaster Engineering Society and its associated student groups. It enables users to submit, review, and track reimbursement requests, create budgets, and manage expenses in a transparent, auditable, and efficient manner. This V\&V Plan outlines the verification and validation processes for ensuring the accuracy, functionality, and security of the software.

\subsection{Objectives}

The primary objective of this V\&V Plan is to confirm that the MES Custom Financial Expense Reporting Platform meets its requirements and operates as intended. The plan aims to:
\begin{itemize}
    \item Build confidence in the software's functional correctness and ensure it meets all outlined requirements.
    \item Validate usability by demonstrating a clear, intuitive, and user-friendly interface for all key user roles.
    \item Ensure system security and data integrity, especially regarding sensitive financial data and authorization levels.
\end{itemize}

The plan does not prioritize nonfunctional aspects such as aesthetic design consistency or performance optimizations beyond standard system requirements, as these are not critical to initial functionality and budget constraints.

\subsection{Challenge Level and Extras}

The challenge level for this project is classified as \textbf{General}, aligning with the expectations outlined in the problem statement. The team has chosen to address the critical needs of verification of functionality, security, and usability. No additional extras beyond the required project scope are included in this V\&V Plan.

\subsection{Relevant Documentation}

The following documents are relevant to this V\&V plan and provide a basis for verification activities:

\begin{itemize}
    \item \textbf{Software Requirements Specification (SRS)} \citep{SRS}: Defines the functional and nonfunctional requirements of the system, serving as a primary source for validation and test case development.
    \item \textbf{Hazard Analysis}\citep{HazardAnalysis}: Outlines potential system risks and serves as a guide for security testing and fault tolerance validation.
    \item \textbf{Problem Statement}\citep{ProblemStatement}: Provides an overview of the project's purpose, goals, and core functionality requirements, detailing the needs of the McMaster Engineering Society for a streamlined financial reporting platform. It identifies key problems in the current system, justifying the need for an automated, digital solution to enhance efficiency and reliability in financial processes.
    \item \textbf{Development Plan}\citep{DevelopmentPlan}: Outlines the project's development lifecycle, including phases for requirements gathering, design, implementation, and testing. It establishes milestones, assigns roles, and defines the timeline, while ensuring that the team follows best practices in Agile development, with iterative check-ins for verification and validation of functional requirements.
\end{itemize}

These documents establish the foundation for testing activities, allowing each verification and validation activity to target specific functional areas, requirements, or potential risk zones.

\section{Plan}
This section outlines the Verification and Validation (V\&V) process for the MES Custom Financial Expense Reporting Platform, which aims to provide reliable financial management for MES and its associated student groups. The plan is structured to cover team roles, verification of the Software Requirements Specification (SRS), design verification, and verification of the V\&V plan itself. Each subsection provides an approach tailored to the critical functional and non-functional requirements outlined in the SRS.

\subsection{Verification and Validation Team}
The V\&V team is composed of designated members, each assigned specific responsibilities to ensure thorough verification of functionality, usability, security, and data integrity. The table below describes each team member's role and responsibilities:

\begin{tabularx}{\textwidth}{|X|X|X|}
    \hline
    \textbf{Name} & \textbf{Role} & \textbf{Responsibilities} \\
    \hline
    Housam & V\&V Lead & Oversees all V\&V activities, ensures that testing is aligned with MES requirements, and coordinates communication between team members and stakeholders. Reviews testing documentation for completeness and accuracy. \\
    \hline
    Rachid & Functional Tester & Manages the testing of all functional requirements, such as reimbursement submission, payment tracking, and budget management. Ensures that each feature meets specified criteria and conducts integration tests for module interactions. \\
    \hline
    Taaha & Security Tester & Responsible for verifying data security and access controls. Conducts tests to validate role-based access, secure data handling, and compliance with MES privacy policies. Implements static and dynamic security tests. \\
    \hline
    Sufyan & Usability Tester & Focuses on the user experience by conducting accessibility testing, analyzing interface consistency, and evaluating navigation efficiency. Engages with end-users to gather feedback and adjust usability requirements. \\
    \hline
  \end{tabularx}

\pagebreak

  \begin{tabularx}{\textwidth}{|X|X|X|}
    Omar & Performance Tester & Conducts tests to evaluate system performance under various loads and conditions. Responsible for stress testing, measuring response times, and identifying any performance bottlenecks, ensuring the platform remains efficient and responsive even during peak usage. \\
    \hline
\end{tabularx}

Each team member will participate in weekly meetings to discuss V\&V progress, address issues, and adapt the V\&V approach as needed. The V\&V Lead will compile findings from each member and provide consolidated feedback to the project supervisor and stakeholders for continuous improvement.

\subsection{SRS Verification Plan}
The SRS document is critical for establishing a clear and agreed-upon set of requirements for the MES Custom Financial Expense Reporting Platform. Our SRS verification plan will ensure the SRS’s accuracy, completeness, and alignment with MES’s objectives. The following approaches will be used for verifying the SRS:

\begin{itemize}
    \item \textbf{Peer Review}: Team members and classmates will conduct an initial peer review of the SRS document. Reviewers will assess the clarity and completeness of requirements, noting ambiguities or missing details. Feedback will be documented, and revisions will be made to address identified issues.
    
    \item \textbf{Stakeholder Review}: The SRS will be presented to MES stakeholders in a structured review meeting. During this meeting, we will walk stakeholders through each requirement, prioritizing critical functionality such as reimbursement processing, budget management, and notification systems. Specific feedback will be sought on:
    \begin{itemize}
        \item Completeness of functional requirements, such as the need for real-time updates on reimbursement statuses.
        \item User accessibility and usability, focusing on ease of submission for student leaders.
        \item Compliance with MES financial policies, including secure audit trails and access controls.
    \end{itemize}
    
    \item \textbf{Supervisor Feedback}: The project supervisor will review the SRS as part of a task-based inspection. The supervisor will be provided with a checklist of essential aspects to verify, including:
    \begin{itemize}
        \item Consistency in terminology and definitions.
        \item Logical flow between sections, ensuring that each requirement aligns with MES’s needs.
        \item Identification of non-functional requirements related to performance, scalability, and security.
    \end{itemize}
    Feedback from the supervisor will be reviewed, and adjustments will be made to enhance clarity and accuracy.

    \item \textbf{SRS Checklist}: A checklist will be created to ensure all requirements are accounted for. The checklist will include criteria such as:
    \begin{itemize}
        \item Traceability: Each requirement must map back to a specific use case or MES need.
        \item Testability: Each requirement must be written in a way that makes it possible to validate through specific tests.
        \item Clarity and conciseness: Requirements should be free from ambiguous language and be concise enough for straightforward implementation.
    \end{itemize}
\end{itemize}

\subsection{Design Verification Plan}
To confirm that the design aligns with the SRS and will fulfill MES’s functional and non-functional requirements, a structured design verification process will be conducted. The design verification plan includes the following elements:

\begin{itemize}
    \item \textbf{Design Walkthroughs}: The team will conduct regular design walkthroughs, reviewing key components and workflows. During these sessions, each module (e.g., reimbursement submission, audit logging, budget tracking) will be analyzed against the SRS to ensure alignment. Feedback from walkthroughs will be documented, and modifications will be made to address any identified issues.

    \item \textbf{Peer Reviews}: Team members will conduct detailed reviews of each other’s design documentation, checking for adherence to design principles, completeness, and alignment with MES’s goals. This review will include:
    \begin{itemize}
        \item Checking that all use cases are addressed in the design.
        \item Verifying that interactions between modules support seamless user experience and efficient functionality.
        \item Ensuring that security measures are in place for handling sensitive financial data.
    \end{itemize}

    \item \textbf{Checklists for Design Verification}: Each design element will be assessed using a checklist, covering:
    \begin{itemize}
        \item Consistency with SRS requirements.
        \item Scalability and extensibility of the design.
        \item Feasibility of implementation within the project timeline and resources.
        \item Compliance with MES standards, such as maintaining an intuitive user interface.
    \end{itemize}
    Checklists will help maintain thoroughness and consistency across the team’s design review efforts.

    \item \textbf{Supervisor Feedback}: After each design phase, the design documentation will be reviewed by the project supervisor. The supervisor’s review will focus on verifying that the design meets project expectations for functionality, scalability, and security. A checklist will be provided to the supervisor to guide the review, focusing on aspects such as:
    \begin{itemize}
        \item Alignment of the design with SRS requirements.
        \item Feasibility of the design for expected usage levels by MES stakeholders.
        \item Identification of any areas where design adjustments are needed to meet MES standards.
    \end{itemize}
    Feedback from the supervisor will be integrated into the design documentation.
\end{itemize}

\subsection{Verification and Validation Plan Verification Plan}
The V\&V plan itself requires verification to ensure that it is both feasible and comprehensive. This verification process will involve the following steps:

\begin{itemize}
    \item \textbf{Peer Review}: Team members will review the V\&V plan for logical flow, completeness, and feasibility. The reviewers will check that each component of the V\&V plan aligns with the requirements in the SRS and that planned activities are feasible within the project timeline.

    \item \textbf{Mutation Testing}: Selected sections of the V\&V plan will undergo mutation testing to identify weaknesses or gaps in test case descriptions. This will help us refine test cases to ensure they cover a wide range of possible inputs and edge cases, making the V\&V plan more robust.

    \item \textbf{Checklist for V\&V Plan Review}: A checklist will guide the review of the V\&V plan, covering:
    \begin{itemize}
        \item Presence of all required V\&V components, such as unit, integration, and system tests.
        \item Feasibility of planned activities, given project resources and constraints.
        \item Traceability between test cases and SRS requirements, ensuring comprehensive coverage.
        \item Alignment of verification techniques with the specific needs of the MES platform.
    \end{itemize}

    \item \textbf{Feedback from Supervisor and Stakeholders}: A meeting will be held with the project supervisor and MES stakeholders to review the V\&V plan. During this session, we will gather feedback on:
    \begin{itemize}
        \item Adequacy of planned testing for mission-critical functions (e.g., reimbursement submission, audit logging).
        \item Planned usability testing to ensure user satisfaction among student groups and administrators.
        \item Risk mitigation strategies for potential project challenges, such as data security concerns.
    \end{itemize}
    The feedback gathered will be used to refine the V\&V plan further, ensuring it meets the expectations of all stakeholders.
\end{itemize}

By implementing the above V\&V plan, we aim to build confidence in the MES Custom Financial Expense Reporting Platform’s functionality, usability, and security, ensuring it meets the expectations of the MES and its users.

\subsection{Implementation Verification Plan}
The implementation of the verification plan will be accomplished through a multi faceted approach:

\subsubsection*{Code Reviews}
\begin{itemize}
  \item All code changes will undergo peer review through GitHub Pull Requests before being merged into the main branch.
  \item At least one team member has to approve the pull request before it can be merged.
  \item Code reviews will use a standardized checklist focusing on:
  \begin{itemize}
    \item Implementation correctness against requirements.
    \item Security considerations for financial data handling.
    \item Test coverage verification.
    \item Error handling completeness.
  \end{itemize}
\end{itemize}

\subsubsection*{Test Execution}
\begin{itemize}
  \item System tests as detailed in Section 4, with particular focus on:
  \begin{itemize}
    \item Functional tests for reimbursement submission (Section 4.1).
    \item Security and data integrity tests (Section 4.2).
    \item Performance and scalability tests (Section 4.2).
  \end{itemize}
  \item Unit tests as outlined in Section 5, covering:
  \begin{itemize}
    \item Individual module testing.
    \item Integration points between components.
    \item Edge cases and error conditions.
  \end{itemize}
\end{itemize}

\subsection*{Static Analysis}
\begin{itemize}
  \item ESLint will be used for static code analysis.
  \item Type checking through TypeScript's compiler.
  \item SonarQube will be used for code quality and security analysis.
\end{itemize}

\subsection*{Review Sessions}
\begin{itemize}
  \item Weekly review sessions will be held with the team to discuss progress, issues, and verification results. This meeting will additionally be held with the project supervisor on a monthly basis.
  \item Review sessions will include a demonstration of new features and verification results.
  \item Occasionally review sessions will include a demonstration of the system to stakeholders.
  \item The checkpoint meetings with the TA will also serve as a review session.
  \item The final presentation will serve as a comprehensive code walkthrough.
\end{itemize}

\subsection{Automated Testing and Verification Tools}
The following tools will be utilized for automated testing and verification, aligned with our development technology stack:

\subsubsection*{Testing Frameworks}
\begin{itemize}
    \item Jest: Primary testing framework for unit and integration testing.
    \item React Testing Library: Component testing focusing on user interactions and UI behavior.
    \item Cypress: End-to-end testing for user flows and system integration.
\end{itemize}

\subsubsection*{Code Quality Tools}
\begin{itemize}
    \item ESLint: Configured with specific rules for TypeScript and Next.js development.
    \item Prettier: Automated code formatting on commit to maintain consistent style.
    \item TypeScript: Static type checking with strict mode enabled.
    \item Git pre-commit hooks: Automated formatting and linting checks before commits are allowed.
\end{itemize}

\subsubsection*{Continuous Integration}
\begin{itemize}
    \item GitHub Actions for:
    \begin{itemize}
        \item Automated test execution on each commit.
        \item Code quality checks through ESLint.
        \item Type checking through TypeScript compiler.
        \item Automated deployment to staging environment for pull requests.
    \end{itemize}
    \item Test coverage reporting through Jest with:
    \begin{itemize}
        \item Minimum 90\% coverage requirement as specified in the Development Plan.
        \item Coverage reports generated for each pull request.
        \item Blocking of merges if coverage thresholds aren't met.
    \end{itemize}
\end{itemize}

\subsubsection*{Performance Monitoring}
\begin{itemize}
    \item Lighthouse: Performance, accessibility, and best practices monitoring.
    \item Chrome DevTools: Memory usage and rendering performance analysis.
    \item Next.js Analytics: Built-in performance monitoring for production deployments.
\end{itemize}

\subsubsection*{Security Testing}
\begin{itemize}
    \item npm audit: Regular dependency vulnerability scanning.
    \item Automated vulnerability scanning integrated into CI pipeline.
\end{itemize}

All tools have been selected to integrate seamlessly with our Next.js and TypeScript stack while supporting our coding standards and quality requirements. The automation of these tools through our CI/CD pipeline ensures consistent verification across all stages of development.

\subsection{Software Validation Plan}
The validation plan ensures the software meets the needs of the McMaster Engineering Society and its 60 student groups through a comprehensive approach:
\subsubsection*{External Data Validation}
\begin{itemize}
   \item Historical reimbursement data from the MES's previous Excel-based system will be used to validate:
   \begin{itemize}
       \item Accuracy of financial calculations.
       \item Correct budget categorization.
       \item Processing time improvements.
   \end{itemize}
   \item Sample receipts provided by the MES will be used to validate the receipt processing system.
\end{itemize}

\subsubsection*{Stakeholder Requirements Review}
\begin{itemize}
   \item Task-based inspection sessions with MES VP Finance to verify SRS completeness:
   \begin{itemize}
       \item Walkthrough of common reimbursement scenarios.
       \item Review of financial approval workflows.
       \item Verification of audit requirements.
   \end{itemize}
   \item Requirements validation through prototype demonstrations:
   \begin{itemize}
       \item Interactive sessions with student group leaders.
       \item Feedback collection on user interface design.
       \item Verification of notification preferences.
   \end{itemize}
\end{itemize}

\subsubsection*{Rev 0 Demo Validation}
\begin{itemize}
   \item Scheduled demonstration with external supervisor (MES VP Finance).
   \item Focus areas for supervisor feedback:
   \begin{itemize}
       \item Validation of implemented requirements from Section 9.1 of the SRS.
       \item Verification of financial workflow accuracy.
       \item Assessment of security measures.
       \item User interface evaluation.
   \end{itemize}
   \item This feedback will be documented and incorporated into subsequent development iterations.
\end{itemize}

\subsubsection*{User Acceptance Testing}
\begin{itemize}
   \item Beta testing program with selected student groups:
   \begin{itemize}
       \item Testing with real reimbursement scenarios.
       \item Validation of notification systems.
       \item Performance under typical usage patterns.
   \end{itemize}
   \item Specific validation metrics:
   \begin{itemize}
       \item Reimbursement request completion time.
       \item Budget categorization accuracy (target: 90\%).
       \item System response times under load.
       \item User error rates during submission process.
   \end{itemize}
\end{itemize}

\subsubsection*{Requirements Validation}
\begin{itemize}
   \item Regular requirement review sessions to ensure:
   \begin{itemize}
       \item Alignment with MES financial policies.
       \item Compliance with security requirements.
       \item Meeting of performance targets.
   \end{itemize}
   \item Validation of specific SRS requirements:
   \begin{itemize}
       \item Custom budget creation functionality.
       \item Real-time ledger tracking.
       \item Multi-level approval workflows.
       \item Automated notifications.
   \end{itemize}
\end{itemize}

All validation activities will be documented and tracked through GitHub. Feedback will be incorporated into the development cycle. There will be particular attention to the core requirements of reducing wait times and preventing loss of reimbursement requests. Any changes to requirements identified through validation will be reviewed and updated in the SRS document accordingly.

\section{System Tests}

\subsection{Tests for Functional Requirements}

\textbf{Test 1: Submission Confirmation}
\begin{itemize}
    \item \textbf{Test ID}: test-id1
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}: User logged in, valid reimbursement document available
    \item \textbf{Input}: A completed reimbursement form with attached receipt
    \item \textbf{Output}: Confirmation message and ledger update
    \item \textbf{Test Case Derivation}: Ensures the form submission logic updates the ledger correctly, as described in the SRS
    \item \textbf{How Test Will Be Performed}: Submit the form and verify through the UI and database that the ledger is updated
\end{itemize}

\textbf{Test 2: Approval Workflow}
\begin{itemize}
    \item \textbf{Test ID}: test-id2
    \item \textbf{Control}: Manual
    \item \textbf{Initial State}: Admin logged in, pending request available
    \item \textbf{Input}: An admin approval action on a pending reimbursement request
    \item \textbf{Output}: Status changes to approved, and notifications are sent
    \item \textbf{Test Case Derivation}: Derived from multi-level approval requirements to ensure workflow consistency
    \item \textbf{How Test Will Be Performed}: Approve the request and check the status change, notification delivery, and database integrity
\end{itemize}

\textbf{Test 3: Expense Addition}
\begin{itemize}
    \item \textbf{Test ID}: test-id3
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}: Initialized budget with no transactions
    \item \textbf{Input}: An expense entry into the system
    \item \textbf{Output}: Budget overview reflects the new expense
    \item \textbf{Test Case Derivation}: Ensures real-time updates to the budget, as required by the SRS
    \item \textbf{How Test Will Be Performed}: Add an expense and verify updates in the UI and database
\end{itemize}
\textbf{Test 4: Budget Creation and Categorization}
\begin{itemize}
    \item \textbf{Test ID}: test-id4
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}: User logged in with access to budget management tools
    \item \textbf{Input}: Budget creation request specifying category and amount
    \item \textbf{Output}: Confirmation of successful budget creation, correctly categorized under specified category
    \item \textbf{Test Case Derivation}: Ensures budget creation and categorization align with SRS requirements for custom budget management
    \item \textbf{How Test Will Be Performed}: Create budget, categorize it, and verify correct display and categorization in dashboard
\end{itemize}

\textbf{Test 5: Notification on Reimbursement Status Change}
\begin{itemize}
    \item \textbf{Test ID}: test-id5
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}: User has a pending reimbursement request
    \item \textbf{Input}: Admin updates status of reimbursement request to "approved"
    \item \textbf{Output}: Notification is sent to user confirming approval, with update in dashboard
    \item \textbf{Test Case Derivation}: Confirms notification system reliability as per SRS notification requirements
    \item \textbf{How Test Will Be Performed}: Change status in admin panel, observe notification and dashboard update for the user
\end{itemize}
\textbf{Test 6: Audit Trail Logging}
\begin{itemize}
    \item \textbf{Test ID}: test-id6
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}: System is idle with no recent actions in audit log
    \item \textbf{Input}: User performs actions, such as budget creation and expense submission
    \item \textbf{Output}: Each action is logged in the audit trail with accurate timestamps and user IDs
    \item \textbf{Test Case Derivation}: Ensures all actions are traceable for compliance, as per SRS audit requirements
    \item \textbf{How Test Will Be Performed}: Perform actions and verify entries in audit log
\end{itemize}

\subsection{Tests for Nonfunctional Requirements}

\textbf{Test 7: Response Time Under Load}
\begin{itemize}
    \item \textbf{Test ID}: test-id7
    \item \textbf{Type}: Dynamic
    \item \textbf{Initial State}: System with minimal load
    \item \textbf{Input/Condition}: Simultaneous budget submissions by multiple users
    \item \textbf{Output/Result}: Response time remains within acceptable limits, per performance specifications
    \item \textbf{Test Case Derivation}: Verifies system’s performance under high load, per nonfunctional requirements for speed and latency
    \item \textbf{How Test Will Be Performed}: Use simulated users to submit requests, monitor response times
\end{itemize}

\textbf{Test 8: Security and Access Control}
\begin{itemize}
    \item \textbf{Test ID}: test-id8
    \item \textbf{Control}: Manual
    \item \textbf{Initial State}: Unauthorized user attempting access
    \item \textbf{Input}: Unauthorized access attempt to restricted sections
    \item \textbf{Output}: Access denied, with secure handling of unauthorized access attempt
    \item \textbf{Test Case Derivation}: Confirms that unauthorized access is prevented as per security requirements in SRS
    \item \textbf{How Test Will Be Performed}: Attempt access and verify access denial and logging of the event
\end{itemize}
\textbf{Test 9: Load Handling}
\begin{itemize}
    \item \textbf{Test ID}: test-id9
    \item \textbf{Type}: Dynamic
    \item \textbf{Initial State}: User dashboard loaded with multiple active sessions
    \item \textbf{Input/Condition}: Simultaneous form submissions from multiple users
    \item \textbf{Output/Result}: System responds within acceptable limits without degradation in performance
    \item \textbf{Test Case Derivation}: Verifies system’s load handling capacity to ensure scalability, per nonfunctional requirements
    \item \textbf{How Test Will Be Performed}: Manually simulate multiple users by having several people submit forms at the same time from different devices or browsers. Each user will fill out and submit a form concurrently while monitoring the system's response time, performance, and any issues that arise, such as slowdowns or errors.
\end{itemize}

\textbf{Test 10: Data Integrity and Security}
\begin{itemize}
    \item \textbf{Test ID}: test-id10
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}: Data integrity checks enabled
    \item \textbf{Input}: Multiple transactions and database updates
    \item \textbf{Output}: Data remains consistent, secure, and unaltered without authorization
    \item \textbf{Test Case Derivation}: Ensures data integrity and protection against unauthorized changes as per security and data integrity requirements
    \item \textbf{How Test Will Be Performed}: Perform series of database operations and validate data consistency and security logs
\end{itemize} 

\subsection{Traceability Between Test Cases and Requirements (above)}

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|}
      \hline
      \textbf{Requirement} & \textbf{Test Case(s)} \\
      \hline
      Reimbursement Submission & test-id1, test-id2 \\
      Budget Management & test-id3, test-id4 \\
      Notification System & test-id5 \\
      Audit Logging & test-id6 \\
      Performance Under Load & test-id7, test-id9 \\
      Security and Access Control & test-id8 \\
      Data Integrity and Security & test-id10 \\
      \hline
  \end{tabular}
  \caption{Traceability Between Test Cases and Requirements}
\end{table}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

N/A

\subsection{Symbolic Parameters}

N/A

\subsection{Usability Survey Questions?}

N/A

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? \\
  \textbf{Sufyan:} What went well for me this assignment was getting github actions to build the pdf on commit. I also enjoyed learning about all the tooling we could use for the project to ensure code quality, test coverage and security. \\
  \textbf{Taaha} What went well while writing this deliverable was the amount of support I had when asking teammates questions. Was extremely helpful whenever I came across an issue I was unsure of. \\
  \textbf{Omar:} What went well was the clarity of this document, we understand our project so we were able to devise this document with little to no questions asked to the TA. \\
  \textbf{Housam:} One aspect that went well for me was refining the plan structure to align with the project’s goals and requirements. Working on the document also helped solidify my understanding of the V\&V process and how it integrates into the larger project goals. \\
  \textbf{Rachid:} What went well for me this deliverable was that we discussed how we were going to work on this very early and this allowed me to spend more time thinking and working on this deliverable

  \item What pain points did you experience during this deliverable, and how
    did you resolve them? \\
  \textbf{Sufyan:} The pain points I experienced were getting the github actions to work properly. I had to do a lot of research and trial and error to get it to work. I also had some issues with the formatting of the document. I resolved these issues by asking for help from my team members. \\
  \textbf{Taaha} The pain points I experienced were mostly related to formatting the document correctly. In order to resolve this, I got help from teammates and looked online. \\
  \textbf{Omar:} I got really sick during this deliverable which slowed down my progress a lot, in addition, it was harder to manage this deliverable since I had a bunch of other deliverables due near the same time. \\
  \textbf{Housam:} The main challenge I faced was balancing detail with conciseness, as I wanted to cover each point in the plan without making it overly lengthy. At times, I felt I was going into too much detail, so I reviewed my work multiple times to find a clear and concise way to present information. Discussing with teammates helped refine certain sections and focus on essential content. \\
  \textbf{Rachid:} This deliverable was extremely clear and as a result, I experienced no pain points.

  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member. 


    
    \textbf{What knowledge and skills will the team collectively need to acquire to successfully complete the verification and validation of your project?}
    
    Key skills needed for V\&V include:
    
    \begin{itemize}
        \item \textbf{Dynamic Testing:} Using Jest and Cypress for unit and integration testing.
        \item \textbf{Static Analysis:} Using ESLint and SonarQube to identify code issues.
        \item \textbf{CI/CD Setup:} Configuring GitHub Actions to automate testing and deployment.
        \item \textbf{Security Testing:} Implementing npm audit for vulnerability scanning.
        \item \textbf{Usability Testing:} Conducting accessibility and usability tests using Lighthouse.
    \end{itemize}

  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
  
  \begin{itemize}
    \item \textbf{Dynamic Testing:} Team tutorials on Jest/Cypress and applying tests in the project, led by Housam.
    \item \textbf{Static Analysis:} Reviewing ESLint/SonarQube documentation and integrating tools during development, led by Omar.
    \item \textbf{CI/CD Setup:} Using GitHub documentation and incremental setup of GitHub Actions, led by Sufyan.
    \item \textbf{Security Testing:} Researching secure coding and using npm audit, led by Taaha.
    \item \textbf{Usability Testing:} Exploring Lighthouse and gathering user feedback, coordinated by Rachid.
\end{itemize}

\end{enumerate}

\end{document}