\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Nov 4, 2024 & 1.1 & Housam: Add parts to section 3 (plan)\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
N/A

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{3cm}|p{10cm}|}
    \hline
    \textbf{Abbreviation} & \textbf{Description} \\
    \hline
    MES & McMaster Engineering Society \\
    \hline
    V\&V & Verification and Validation \\
    \hline
    SRS & Software Requirements Specification \\
    \hline
    UI & User Interface \\
    \hline
    API & Application Programming Interface \\
    \hline
    CI/CD & Continuous Integration / Continuous Deployment \\
    \hline
\end{tabular}

\bigskip

This section defines the symbols, abbreviations, and acronyms used throughout the document to ensure clarity and consistency.


\newpage

\pagenumbering{arabic}

This document outlines the Verification and Validation (V\&V) Plan for the McMaster Engineering Society Custom Financial Expense Reporting Platform. The V\&V plan will ensure that the software performs as expected, meets user requirements, is secure, and reliable. The primary goals of this document are to define the objectives, methods, and scope of the V\&V process, covering aspects such as requirements validation, design verification, and system testing.

\section{General Information}

\subsection{Summary}

The MES Custom Financial Expense Reporting Platform is a web-based application designed to streamline financial operations for McMaster Engineering Society and its associated student groups. It enables users to submit, review, and track reimbursement requests, create budgets, and manage expenses in a transparent, auditable, and efficient manner. This V\&V Plan outlines the verification and validation processes for ensuring the accuracy, functionality, and security of the software.

\subsection{Objectives}

The primary objective of this V\&V Plan is to confirm that the MES Custom Financial Expense Reporting Platform meets its requirements and operates as intended. The plan aims to:
\begin{itemize}
    \item Build confidence in the software's functional correctness and ensure it meets all outlined requirements.
    \item Validate usability by demonstrating a clear, intuitive, and user-friendly interface for all key user roles.
    \item Ensure system security and data integrity, especially regarding sensitive financial data and authorization levels.
\end{itemize}

The plan does not prioritize nonfunctional aspects such as aesthetic design consistency or performance optimizations beyond standard system requirements, as these are not critical to initial functionality and budget constraints.

\subsection{Challenge Level and Extras}

\begin{enumerate}
  \item Useability Testing \\
  Conduct formal usability testing with a variety of stakeholders (students, financial staff) to refine the interface and ensure a smooth user experience.
  \item User Documentation \\
  Create comprehensive user documentation in the form of written documentation and video tutorials that guides end-users through every step of submitting expenses, reviewing budgets, and navigating the platform.
\end{enumerate}

The challenge level for this project is classified as \textbf{General}, aligning with the expectations outlined in the problem statement. The team has chosen to address the critical needs of verification of functionality, security, and usability. No additional extras beyond the required project scope are included in this V\&V Plan.

\subsection{Relevant Documentation}

The following documents are relevant to this V\&V plan and provide a basis for verification activities:

\begin{itemize}
    \item \textbf{Software Requirements Specification (SRS)} \citep{SRS}: Defines the functional and nonfunctional requirements of the system, serving as a primary source for validation and test case development.
    \item \textbf{Hazard Analysis}\citep{HazardAnalysis}: Outlines potential system risks and serves as a guide for security testing and fault tolerance validation.
    \item \textbf{Problem Statement}\citep{ProblemStatement}: Provides an overview of the project's purpose, goals, and core functionality requirements, detailing the needs of the McMaster Engineering Society for a streamlined financial reporting platform. It identifies key problems in the current system, justifying the need for an automated, digital solution to enhance efficiency and reliability in financial processes.
    \item \textbf{Development Plan}\citep{DevelopmentPlan}: Outlines the project's development lifecycle, including phases for requirements gathering, design, implementation, and testing. It establishes milestones, assigns roles, and defines the timeline, while ensuring that the team follows best practices in Agile development, with iterative check-ins for verification and validation of functional requirements.
\end{itemize}

These documents establish the foundation for testing activities, allowing each verification and validation activity to target specific functional areas, requirements, or potential risk zones.

\section{Plan}
This section outlines the Verification and Validation (V\&V) process for the MES Custom Financial Expense Reporting Platform, which aims to provide reliable financial management for MES and its associated student groups. The plan is structured to cover team roles, verification of the Software Requirements Specification (SRS), design verification, and verification of the V\&V plan itself. Each subsection provides an approach tailored to the critical functional and non-functional requirements outlined in the SRS.

\subsection{Verification and Validation Team}
The V\&V team is composed of designated members, each assigned specific responsibilities to ensure thorough verification of functionality, usability, security, and data integrity. The table below describes each team member's role and responsibilities:

\begin{tabularx}{\textwidth}{|X|X|X|}
    \hline
    \textbf{Name} & \textbf{Role} & \textbf{Responsibilities} \\
    \hline
    Housam & V\&V Lead & Oversees all V\&V activities, ensures that testing is aligned with MES requirements, and coordinates communication between team members and stakeholders. Reviews testing documentation for completeness and accuracy. \\
    \hline
    Rachid & Functional Tester & Manages the testing of all functional requirements, such as reimbursement submission, payment tracking, and budget management. Ensures that each feature meets specified criteria and conducts integration tests for module interactions. \\
    \hline
    Taaha & Security Tester & Responsible for verifying data security and access controls. Conducts tests to validate role-based access, secure data handling, and compliance with MES privacy policies. Implements static and dynamic security tests. \\
    \hline
    Sufyan & Usability Tester & Focuses on the user experience by conducting accessibility testing, analyzing interface consistency, and evaluating navigation efficiency. Engages with end-users to gather feedback and adjust usability requirements. \\
    \hline
  \end{tabularx}

\pagebreak

  \begin{tabularx}{\textwidth}{|X|X|X|}
    Omar & Performance Tester & Conducts tests to evaluate system performance under various loads and conditions. Responsible for stress testing, measuring response times, and identifying any performance bottlenecks, ensuring the platform remains efficient and responsive even during peak usage. \\
    \hline
\end{tabularx}

Each team member will participate in weekly meetings to discuss V\&V progress, address issues, and adapt the V\&V approach as needed. The V\&V Lead will compile findings from each member and provide consolidated feedback to the project supervisor and stakeholders for continuous improvement.

\subsection{SRS Verification Plan}
The SRS document is critical for establishing a clear and agreed-upon set of requirements for the MES Custom Financial Expense Reporting Platform. Our SRS verification plan will ensure the SRS’s accuracy, completeness, and alignment with MES’s objectives. The following approaches will be used for verifying the SRS:

\begin{itemize}
    \item \textbf{Peer Review}: Team members and classmates will conduct an initial peer review of the SRS document. Reviewers will assess the clarity and completeness of requirements, noting ambiguities or missing details. Feedback will be documented, and revisions will be made to address identified issues.
    
    \item \textbf{Stakeholder Review}: The SRS will be presented to MES stakeholders in a structured review meeting. During this meeting, we will walk stakeholders through each requirement, prioritizing critical functionality such as reimbursement processing, budget management, and notification systems. Specific feedback will be sought on:
    \begin{itemize}
        \item Completeness of functional requirements, such as the need for real-time updates on reimbursement statuses.
        \item User accessibility and usability, focusing on ease of submission for student leaders.
        \item Compliance with MES financial policies, including secure audit trails and access controls.
    \end{itemize}
    
    \item \textbf{Supervisor Feedback}: The project supervisor will review the SRS as part of a task-based inspection. The supervisor will be provided with a checklist of essential aspects to verify, including:
    \begin{itemize}
        \item Consistency in terminology and definitions.
        \item Logical flow between sections, ensuring that each requirement aligns with MES’s needs.
        \item Identification of non-functional requirements related to performance, scalability, and security.
    \end{itemize}
    Feedback from the supervisor will be reviewed, and adjustments will be made to enhance clarity and accuracy.

    \item \textbf{SRS Checklist}: A checklist will be created to ensure all requirements are accounted for. The checklist will include criteria such as:
    \begin{itemize}
        \item Traceability: Each requirement must map back to a specific use case or MES need.
        \item Testability: Each requirement must be written in a way that makes it possible to validate through specific tests.
        \item Clarity and conciseness: Requirements should be free from ambiguous language and be concise enough for straightforward implementation.
    \end{itemize}
\end{itemize}

\subsection{Design Verification Plan}
To confirm that the design aligns with the SRS and will fulfill MES’s functional and non-functional requirements, a structured design verification process will be conducted. The design verification plan includes the following elements:

\begin{itemize}
    \item \textbf{Design Walkthroughs}: The team will conduct regular design walkthroughs, reviewing key components and workflows. During these sessions, each module (e.g., reimbursement submission, audit logging, budget tracking) will be analyzed against the SRS to ensure alignment. Feedback from walkthroughs will be documented, and modifications will be made to address any identified issues.

    \item \textbf{Peer Reviews}: Team members will conduct detailed reviews of each other’s design documentation, checking for adherence to design principles, completeness, and alignment with MES’s goals. This review will include:
    \begin{itemize}
        \item Checking that all use cases are addressed in the design.
        \item Verifying that interactions between modules support seamless user experience and efficient functionality.
        \item Ensuring that security measures are in place for handling sensitive financial data.
    \end{itemize}

    \item \textbf{Checklists for Design Verification}: Each design element will be assessed using a checklist, covering:
    \begin{itemize}
        \item Consistency with SRS requirements.
        \item Scalability and extensibility of the design.
        \item Feasibility of implementation within the project timeline and resources.
        \item Compliance with MES standards, such as maintaining an intuitive user interface.
    \end{itemize}
    Checklists will help maintain thoroughness and consistency across the team’s design review efforts.

    \item \textbf{Supervisor Feedback}: After each design phase, the design documentation will be reviewed by the project supervisor. The supervisor’s review will focus on verifying that the design meets project expectations for functionality, scalability, and security. A checklist will be provided to the supervisor to guide the review, focusing on aspects such as:
    \begin{itemize}
        \item Alignment of the design with SRS requirements.
        \item Feasibility of the design for expected usage levels by MES stakeholders.
        \item Identification of any areas where design adjustments are needed to meet MES standards.
    \end{itemize}
    Feedback from the supervisor will be integrated into the design documentation.
\end{itemize}

\subsection{Verification and Validation Plan Verification Plan}
The V\&V plan itself requires verification to ensure that it is both feasible and comprehensive. This verification process will involve the following steps:

\begin{itemize}
    \item \textbf{Peer Review}: Team members will review the V\&V plan for logical flow, completeness, and feasibility. The reviewers will check that each component of the V\&V plan aligns with the requirements in the SRS and that planned activities are feasible within the project timeline.

    \item \textbf{Mutation Testing}: Selected sections of the V\&V plan will undergo mutation testing to identify weaknesses or gaps in test case descriptions. This will help us refine test cases to ensure they cover a wide range of possible inputs and edge cases, making the V\&V plan more robust.

    \item \textbf{Checklist for V\&V Plan Review}: A checklist will guide the review of the V\&V plan, covering:
    \begin{itemize}
        \item Presence of all required V\&V components, such as unit, integration, and system tests.
        \item Feasibility of planned activities, given project resources and constraints.
        \item Traceability between test cases and SRS requirements, ensuring comprehensive coverage.
        \item Alignment of verification techniques with the specific needs of the MES platform.
    \end{itemize}

    \item \textbf{Feedback from Supervisor and Stakeholders}: A meeting will be held with the project supervisor and MES stakeholders to review the V\&V plan. During this session, we will gather feedback on:
    \begin{itemize}
        \item Adequacy of planned testing for mission-critical functions (e.g., reimbursement submission, audit logging).
        \item Planned usability testing to ensure user satisfaction among student groups and administrators.
        \item Risk mitigation strategies for potential project challenges, such as data security concerns.
    \end{itemize}
    The feedback gathered will be used to refine the V\&V plan further, ensuring it meets the expectations of all stakeholders.
\end{itemize}

By implementing the above V\&V plan, we aim to build confidence in the MES Custom Financial Expense Reporting Platform’s functionality, usability, and security, ensuring it meets the expectations of the MES and its users.

\subsection{Implementation Verification Plan}
The implementation of the verification plan will be accomplished through a multi faceted approach:

\subsubsection*{Code Reviews}
\begin{itemize}
  \item All code changes will undergo peer review through GitHub Pull Requests before being merged into the main branch.
  \item At least one team member has to approve the pull request before it can be merged.
  \item Code reviews will use a standardized checklist focusing on:
  \begin{itemize}
    \item Implementation correctness against requirements.
    \item Security considerations for financial data handling.
    \item Test coverage verification.
    \item Error handling completeness.
  \end{itemize}
\end{itemize}

\subsubsection*{Test Execution}
\begin{itemize}
  \item System tests as detailed in Section 4, with particular focus on:
  \begin{itemize}
    \item Functional tests for reimbursement submission (Section 4.1).
    \item Security and data integrity tests (Section 4.2).
    \item Performance and scalability tests (Section 4.2).
  \end{itemize}
  \item Unit tests as outlined in Section 5, covering:
  \begin{itemize}
    \item Individual module testing.
    \item Integration points between components.
    \item Edge cases and error conditions.
  \end{itemize}
\end{itemize}

\subsection*{Static Analysis}
\begin{itemize}
  \item ESLint will be used for static code analysis.
  \item Type checking through TypeScript's compiler.
  \item SonarQube will be used for code quality and security analysis.
\end{itemize}

\subsection*{Review Sessions}
\begin{itemize}
  \item Weekly review sessions will be held with the team to discuss progress, issues, and verification results. This meeting will additionally be held with the project supervisor on a monthly basis.
  \item Review sessions will include a demonstration of new features and verification results.
  \item Occasionally review sessions will include a demonstration of the system to stakeholders.
  \item The checkpoint meetings with the TA will also serve as a review session.
  \item The final presentation will serve as a comprehensive code walkthrough.
\end{itemize}

\subsection{Automated Testing and Verification Tools}
The following tools will be utilized for automated testing and verification, aligned with our development technology stack:

\subsubsection*{Testing Frameworks}
\begin{itemize}
    \item Jest: Primary testing framework for unit and integration testing.
    \item React Testing Library: Component testing focusing on user interactions and UI behavior.
    \item Cypress: End-to-end testing for user flows and system integration.
\end{itemize}

\subsubsection*{Code Quality Tools}
\begin{itemize}
    \item ESLint: Configured with specific rules for TypeScript and Next.js development.
    \item Prettier: Automated code formatting on commit to maintain consistent style.
    \item TypeScript: Static type checking with strict mode enabled.
    \item Git pre-commit hooks: Automated formatting and linting checks before commits are allowed.
\end{itemize}

\subsubsection*{Continuous Integration}
\begin{itemize}
    \item GitHub Actions for:
    \begin{itemize}
        \item Automated test execution on each commit.
        \item Code quality checks through ESLint.
        \item Type checking through TypeScript compiler.
        \item Automated deployment to staging environment for pull requests.
    \end{itemize}
    \item Test coverage reporting through Jest with:
    \begin{itemize}
        \item Minimum 90\% coverage requirement as specified in the Development Plan.
        \item Coverage reports generated for each pull request.
        \item Blocking of merges if coverage thresholds aren't met.
    \end{itemize}
\end{itemize}

\subsubsection*{Performance Monitoring}
\begin{itemize}
    \item Lighthouse: Performance, accessibility, and best practices monitoring.
    \item Chrome DevTools: Memory usage and rendering performance analysis.
    \item Next.js Analytics: Built-in performance monitoring for production deployments.
\end{itemize}

\subsubsection*{Security Testing}
\begin{itemize}
    \item npm audit: Regular dependency vulnerability scanning.
    \item Automated vulnerability scanning integrated into CI pipeline.
\end{itemize}

All tools have been selected to integrate seamlessly with our Next.js and TypeScript stack while supporting our coding standards and quality requirements. The automation of these tools through our CI/CD pipeline ensures consistent verification across all stages of development.

\subsection{Software Validation Plan}
The validation plan ensures the software meets the needs of the McMaster Engineering Society and its 60 student groups through a comprehensive approach:
\subsubsection*{External Data Validation}
\begin{itemize}
   \item Historical reimbursement data from the MES's previous Excel-based system will be used to validate:
   \begin{itemize}
       \item Accuracy of financial calculations.
       \item Correct budget categorization.
       \item Processing time improvements.
   \end{itemize}
   \item Sample receipts provided by the MES will be used to validate the receipt processing system.
\end{itemize}

\subsubsection*{Stakeholder Requirements Review}
\begin{itemize}
   \item Task-based inspection sessions with MES VP Finance to verify SRS completeness:
   \begin{itemize}
       \item Walkthrough of common reimbursement scenarios.
       \item Review of financial approval workflows.
       \item Verification of audit requirements.
   \end{itemize}
   \item Requirements validation through prototype demonstrations:
   \begin{itemize}
       \item Interactive sessions with student group leaders.
       \item Feedback collection on user interface design.
       \item Verification of notification preferences.
   \end{itemize}
\end{itemize}

\subsubsection*{Rev 0 Demo Validation}
\begin{itemize}
   \item Scheduled demonstration with external supervisor (MES VP Finance).
   \item Focus areas for supervisor feedback:
   \begin{itemize}
       \item Validation of implemented requirements from Section 9.1 of the SRS.
       \item Verification of financial workflow accuracy.
       \item Assessment of security measures.
       \item User interface evaluation.
   \end{itemize}
   \item This feedback will be documented and incorporated into subsequent development iterations.
\end{itemize}

\subsubsection*{User Acceptance Testing}
\begin{itemize}
   \item Beta testing program with selected student groups:
   \begin{itemize}
       \item Testing with real reimbursement scenarios.
       \item Validation of notification systems.
       \item Performance under typical usage patterns.
   \end{itemize}
   \item Specific validation metrics:
   \begin{itemize}
       \item Reimbursement request completion time.
       \item Budget categorization accuracy (target: 90\%).
       \item System response times under load.
       \item User error rates during submission process.
   \end{itemize}
\end{itemize}

\subsubsection*{Requirements Validation}
\begin{itemize}
   \item Regular requirement review sessions to ensure:
   \begin{itemize}
       \item Alignment with MES financial policies.
       \item Compliance with security requirements.
       \item Meeting of performance targets.
   \end{itemize}
   \item Validation of specific SRS requirements:
   \begin{itemize}
       \item Custom budget creation functionality.
       \item Real-time ledger tracking.
       \item Multi-level approval workflows.
       \item Automated notifications.
   \end{itemize}
\end{itemize}

All validation activities will be documented and tracked through GitHub. Feedback will be incorporated into the development cycle. There will be particular attention to the core requirements of reducing wait times and preventing loss of reimbursement requests. Any changes to requirements identified through validation will be reviewed and updated in the SRS document accordingly.

\section{System Tests}

\subsection{Tests for Functional Requirements}


\textbf{Test 1: Submission Confirmation}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id1}
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}:
    \begin{itemize}
        \item User (``employee'' role) is logged into the system.
        \item User is on the ``Submit Reimbursement'' page. 
        \item No pending reimbursement forms for this user are currently in the system.
    \end{itemize}
    \item \textbf{Input}:
    \begin{enumerate}
        \item Completed reimbursement form with the following fields:
        \begin{itemize}
            \item \textit{Employee ID}: \texttt{EMP-1001}
            \item \textit{Date of Expense}: \texttt{2025-01-15}
            \item \textit{Expense Description}: \texttt{Conference Registration Fee}
            \item \textit{Expense Amount}: \texttt{250.00 USD}
            \item \textit{Category}: \texttt{Professional Development}
            \item \textit{Notes (optional)}: \texttt{Attended annual tech conference}
        \end{itemize}
        \item Attached digital receipt: 
        \begin{itemize}
            \item \textit{File Name}: \texttt{receipt\_conf2025.jpg}
            \item \textit{File Format}: JPEG
            \item \textit{File Size}: Approximately 200 KB
        \end{itemize}
    \end{enumerate}
    \item \textbf{Output}:
    \begin{itemize}
        \item On-screen message: ``Reimbursement form submitted successfully.''
        \item New record inserted into the \texttt{Reimbursements} table (or ledger) with:
        \begin{itemize}
            \item A unique reimbursement ID (e.g., \texttt{RB-2025-0001}).
            \item All form field data.
            \item A link or reference to the attached receipt file.
            \item Time stamp of submission.
        \end{itemize}
    \end{itemize}
    \item \textbf{Test Case Derivation}: 
    Verifies that the form submission logic (per SRS Section~\ref{req-reimbursement-submission}) correctly inserts new reimbursement entries and provides a confirmation.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Fill all required fields in the ``Submit Reimbursement'' form with the specified data.
        \item Upload the receipt image (\texttt{receipt\_conf2025.jpg}).
        \item Click ``Submit.'' 
        \item Check the UI for the success message.
        \item Query the \texttt{Reimbursements} table to ensure a new record is created with the correct details.
    \end{enumerate}
\end{itemize}


\textbf{Test 2: Approval Workflow}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id2}
    \item \textbf{Control}: Manual
    \item \textbf{Initial State}:
    \begin{itemize}
        \item An Administrator (``admin'' role) is logged into the system.
        \item A reimbursement request (e.g., \texttt{RB-2025-0002}) with status = ``Pending'' is already in the \texttt{Reimbursements} table.
    \end{itemize}
    \item \textbf{Input}:
    \begin{itemize}
        \item Administrator selects the pending request \texttt{RB-2025-0002} on the ``Pending Approvals'' dashboard.
        \item Administrator clicks ``Approve'' and optionally enters a comment such as ``Within budget guidelines.''
    \end{itemize}
    \item \textbf{Output}:
    \begin{itemize}
        \item Status of \texttt{RB-2025-0002} changes from ``Pending'' to ``Approved.''
        \item Notification is sent to the requestor (email or in-app).
        \item The \texttt{Reimbursements} table updates with a new status, approval timestamp, and the admin comment (if provided).
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Confirms multi-level approval logic from SRS Section~\ref{req-approval-workflow} functions correctly and that notifications are triggered on status change.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Admin navigates to the ``Pending Approvals'' page.
        \item Admin selects \texttt{RB-2025-0002}, clicks ``Approve'' and enters an optional note.
        \item Admin confirms the status changes to ``Approved'' in the UI.
        \item Admin logs out; the requestor logs in and checks their notification alert or email.
        \item Verify in the database that the record status and timestamp have been updated properly.
    \end{enumerate}
\end{itemize}


\textbf{Test 3: Expense Addition}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id3}
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}:
    \begin{itemize}
        \item Organization's total budget set (e.g., 10,000 USD) with zero expenses recorded.
        \item The database table \texttt{Expenses} is empty.
    \end{itemize}
    \item \textbf{Input}:
    \begin{itemize}
        \item \textit{Expense Name}: \texttt{Office Supplies}
        \item \textit{Date}: \texttt{2025-02-10}
        \item \textit{Amount}: \texttt{120.00 USD}
        \item \textit{Category}: \texttt{Operations}
    \end{itemize}
    \item \textbf{Output}:
    \begin{itemize}
        \item The ``Budget Overview'' UI page shows a new expense of 120.00 USD listed under \texttt{Operations}.
        \item The system deducts 120.00 USD from the total budget display.
        \item A new record is created in the \texttt{Expenses} table with the correct data and a unique ID (e.g., \texttt{EXP-2025-0001}).
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Validates real-time budget updating from SRS Section~\ref{req-expense-tracking}.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Navigate to ``Add Expense'' form.
        \item Input the specified data and click ``Submit.''
        \item Observe the updated ``Budget Overview'' UI reflecting the new total.
        \item Query the \texttt{Expenses} table to confirm a new entry is recorded with the correct fields.
    \end{enumerate}
\end{itemize}


\textbf{Test 4: Budget Creation and Categorization}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id4}
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}:
    \begin{itemize}
        \item User (``finance manager'' role) is logged in.
        \item User has permission to create budgets.
        \item The category to be created (e.g., \texttt{Marketing}) does not exist yet.
    \end{itemize}
    \item \textbf{Input}:
    \begin{itemize}
        \item \textit{Category Name}: \texttt{Marketing}
        \item \textit{Allocated Amount}: \texttt{5000.00 USD}
        \item \textit{Effective Start Date}: \texttt{2025-03-01}
        \item \textit{End Date (optional)}: \texttt{2025-12-31}
        \item \textit{Notes}: \texttt{Annual marketing budget for campaigns}
    \end{itemize}
    \item \textbf{Output}:
    \begin{itemize}
        \item A new entry in the \texttt{Budgets} table with:
        \begin{itemize}
            \item \texttt{Category} = \texttt{Marketing}
            \item \texttt{Amount} = 5000.00
            \item \texttt{Start Date} = 2025-03-01
            \item \texttt{End Date} = 2025-12-31
        \end{itemize}
        \item UI confirmation message: ``Budget created successfully!''
        \item The new budget appears on the ``Budget Dashboard'' under \texttt{Marketing}.
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Ensures new budgets can be created and are properly categorized, as per SRS Section~\ref{req-budget-management}.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Go to the ``Create New Budget'' interface.
        \item Enter the data above and submit.
        \item Verify the success message.
        \item Check the ``Budget Dashboard'' for the new \texttt{Marketing} budget.
        \item Verify a corresponding entry in the \texttt{Budgets} table is present and accurate.
    \end{enumerate}
\end{itemize}


\textbf{Test 5: Notification on Reimbursement Status Change}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id5}
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}:
    \begin{itemize}
        \item Employee has already submitted a reimbursement request \texttt{RB-2025-0003} (status = ``Pending'').
        \item The notification system is configured to send email or in-app messages.
    \end{itemize}
    \item \textbf{Input}:
    \begin{itemize}
        \item Admin changes \texttt{RB-2025-0003} from ``Pending'' to ``Approved'' in the ``Admin Panel'' interface.
    \end{itemize}
    \item \textbf{Output}:
    \begin{itemize}
        \item The request's status changes to ``Approved'' in the system.
        \item An email or in-app notification is sent to the employee: 
        \texttt{``Your reimbursement request RB-2025-0003 has been approved.''}
        \item The employee's dashboard updates to show \texttt{RB-2025-0003} as ``Approved.''
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Confirms the notification subsystem works properly, as per SRS Section~\ref{req-notifications}.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Admin logs into the ``Admin Panel.''
        \item Locates the request \texttt{RB-2025-0003} (status = ``Pending'').
        \item Clicks ``Approve.'' 
        \item Employee checks email or logs in to confirm the new status and sees the notification.
    \end{enumerate}
\end{itemize}

\textbf{Test 6: Audit Trail Logging}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id6}
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}:
    \begin{itemize}
        \item System idle with an empty or known state in the \texttt{AuditLog} table.
        \item Auditing feature is enabled (per configuration).
    \end{itemize}
    \item \textbf{Input}:
    \begin{enumerate}
        \item User (``employee'' role) creates a new budget named \texttt{TempBudget} with 200.00 USD.
        \item Admin (``admin'' role) approves a reimbursement \texttt{RB-2025-0004}.
        \item The same user (``employee'' role) adds an expense under \texttt{TempBudget}.
    \end{enumerate}
    \item \textbf{Output}:
    \begin{itemize}
        \item \texttt{AuditLog} table records each action with:
        \begin{itemize}
            \item Action type (e.g., \texttt{BUDGET\_CREATED}, \texttt{REIMBURSEMENT\_APPROVED}, \texttt{EXPENSE\_ADDED}).
            \item User ID of the actor.
            \item Timestamp of the action.
            \item Reference ID (e.g., \texttt{TempBudget} or \texttt{RB-2025-0004}).
        \end{itemize}
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Ensures traceability of major user actions for compliance and auditing, per SRS Section~\ref{req-audit-logging}.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Verify the last known entry in \texttt{AuditLog}.
        \item Perform each of the actions described above in sequence.
        \item Inspect the \texttt{AuditLog} table to confirm new entries exist with correct timestamps, user IDs, and descriptions.
        \item Ensure the entries are in chronological order and data is accurate.
    \end{enumerate}
\end{itemize}


\subsection{Tests for Nonfunctional Requirements}


\textbf{Test 7: Response Time Under Load}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id7}
    \item \textbf{Type}: Dynamic
    \item \textbf{Initial State}:
    \begin{itemize}
        \item System running with minimal concurrent user load.
        \item Performance monitoring tools in place (e.g., system logs, load test software).
    \end{itemize}
    \item \textbf{Input/Condition}:
    \begin{itemize}
        \item 100 simultaneous submissions of the ``Add Expense'' form from different user accounts within a 30-second window.
        \item Each expense submission includes typical data:
        \begin{itemize}
            \item \textit{Expense Name}: \texttt{LoadTest Expense \#}
            \item \textit{Amount}: Random range from 10 to 50 USD
            \item \textit{Category}: \texttt{Testing}
        \end{itemize}
    \end{itemize}
    \item \textbf{Output/Result}:
    \begin{itemize}
        \item The system responds to each submission within 2 seconds (target maximum) as specified by the performance requirements in SRS Section~\ref{req-performance}.
        \item No significant errors or timeouts occur.
        \item The \texttt{Expenses} table accurately reflects all 100 expense records.
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Validates that under a load of 100 concurrent requests, the system meets the 2-second response time requirement.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Use a load testing tool (e.g., JMeter) to simulate 100 users submitting the form concurrently.
        \item Measure the average and 90th/95th percentile response times.
        \item Confirm no response exceeds 2 seconds.
        \item Verify the system logs and database for any failures or unexpected slowdowns.
    \end{enumerate}
\end{itemize}


\textbf{Test 8: Security and Access Control}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id8}
    \item \textbf{Control}: Manual
    \item \textbf{Initial State}:
    \begin{itemize}
        \item System is running in a secure environment.
        \item A test account with limited privileges is available (e.g., \texttt{USER-TEST-LIMITED}).
        \item A restricted resource or page (e.g., \texttt{/admin/approval}) requires admin access.
    \end{itemize}
    \item \textbf{Input}:
    \begin{itemize}
        \item An unauthorized user (\texttt{USER-TEST-LIMITED}) attempts to access \texttt{/admin/approval}.
    \end{itemize}
    \item \textbf{Output}:
    \begin{itemize}
        \item Access is denied with an HTTP 403 status or an ``Access Denied'' message in the UI.
        \item The unauthorized request is logged in the security logs with timestamp and user ID.
        \item No data or admin functionalities are revealed to the unauthorized user.
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Confirms that role-based access control (RBAC) and security measures align with SRS Section~\ref{req-security}.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Log in as the limited-access user.
        \item Attempt to navigate directly to \texttt{/admin/approval}.
        \item Observe and record the system’s response (expect 403 or an explicit denial page).
        \item Check security logs (e.g., \texttt{SecurityAudit} table) to confirm the attempt was recorded.
    \end{enumerate}
\end{itemize}


\textbf{Test 9: Load Handling}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id9}
    \item \textbf{Type}: Dynamic
    \item \textbf{Initial State}:
    \begin{itemize}
        \item User dashboard is operational, with multiple active user sessions.
        \item The system is running in a typical production-like environment.
    \end{itemize}
    \item \textbf{Input/Condition}:
    \begin{itemize}
        \item 20+ concurrent users each submitting various forms (Expense forms, Reimbursement forms) within a short time frame (e.g., 1 minute).
        \item Each user enters standard data fields, with correct formatting.
    \end{itemize}
    \item \textbf{Output/Result}:
    \begin{itemize}
        \item The system continues to operate without crashes or queue overload.
        \item Average response time does not exceed 2.5 seconds for any user.
        \item All forms are successfully processed and recorded.
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Demonstrates system scalability and stability under moderate multi-user load, per SRS Section~\ref{req-scalability}.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Have multiple testers (or a load test script) submit forms simultaneously.
        \item Monitor the system’s resource usage (CPU, memory) and response times using performance tools.
        \item Verify that forms are processed correctly in the database with no errors or excessive delays.
    \end{enumerate}
\end{itemize}

\textbf{Test 10: Data Integrity and Security}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id10}
    \item \textbf{Control}: Automatic
    \item \textbf{Initial State}:
    \begin{itemize}
        \item The system’s data integrity checks and encryption modules are enabled.
        \item Database transaction logging is active.
    \end{itemize}
    \item \textbf{Input}:
    \begin{itemize}
        \item A series of database updates (budget creation, expense addition, reimbursement approvals).
        \item Simulated unauthorized attempts to modify data directly in the database (e.g., via SQL injection attempt \texttt{' OR 1=1;--}).
    \end{itemize}
    \item \textbf{Output}:
    \begin{itemize}
        \item All legitimate transactions are committed correctly and remain consistent (matching the intended user actions).
        \item Any unauthorized modification attempts are blocked and logged, with no data corruption or compromise.
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Ensures data integrity and security in compliance with SRS Section~\ref{req-data-integrity}, and that unauthorized changes fail safely.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Execute standard workflows that update data (create budget, submit expense, approve reimbursement).
        \item Check the database to verify that each record is accurate and corresponds exactly to user actions.
        \item Attempt a known malicious injection or direct DB manipulation with an unauthorized user.
        \item Confirm that the system rejects the unauthorized action and logs the attempt.
    \end{enumerate}
\end{itemize}

\textbf{Test 11: Usability Evaluation}
\begin{itemize}
    \item \textbf{Test ID}: \texttt{test-id11}
    \item \textbf{Control}: Manual (Survey/Checklist)
    \item \textbf{Initial State}:
    \begin{itemize}
        \item System is deployed to a test environment.
        \item A group of 5--10 representative end-users is recruited for a usability session.
    \end{itemize}
    \item \textbf{Input}:
    \begin{itemize}
        \item Each participant is given a set of typical tasks (e.g., submit a reimbursement, approve a budget, view audit logs).
        \item Participants fill out a short usability checklist (see Appendix~A) with items like:
        \begin{itemize}
            \item Task completion time.
            \item Clarity of UI labels.
            \item Ease of navigation (1--5 scale).
            \item Overall satisfaction (1--5 scale).
        \end{itemize}
    \end{itemize}
    \item \textbf{Output}:
    \begin{itemize}
        \item Quantitative ratings on usability (e.g., average time to complete each task, average satisfaction rating).
        \item Qualitative feedback for UI improvements.
    \end{itemize}
    \item \textbf{Test Case Derivation}:
    Addresses usability requirements (if included in SRS or as an additional quality attribute). Ensures the system meets user experience standards.
    \item \textbf{How Test Will Be Performed}:
    \begin{enumerate}
        \item Conduct a guided user-testing session.
        \item Observe participants as they perform tasks, record errors or delays.
        \item Collect and aggregate checklist responses.
        \item Analyze feedback and identify UI/UX issues.
    \end{enumerate}
\end{itemize}

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[h]
  \centering
  \begin{tabular}{|m{6cm}|m{7cm}|}
      \hline
      \textbf{Requirement (SRS Section)} & \textbf{Test Case(s)} \\
      \hline
      \textbf{Reimbursement Submission} 
        (SRS~\ref{req-reimbursement-submission}) 
      & \texttt{test-id1}, \texttt{test-id2} \\
      \hline
      \textbf{Budget Management} 
        (SRS~\ref{req-budget-management}) 
      & \texttt{test-id3}, \texttt{test-id4} \\
      \hline
      \textbf{Notification System} 
        (SRS~\ref{req-notifications}) 
      & \texttt{test-id5} \\
      \hline
      \textbf{Audit Logging} 
        (SRS~\ref{req-audit-logging}) 
      & \texttt{test-id6} \\
      \hline
      \textbf{Performance Under Load} 
        (SRS~\ref{req-performance}) 
      & \texttt{test-id7}, \texttt{test-id9} \\
      \hline
      \textbf{Security and Access Control} 
        (SRS~\ref{req-security}) 
      & \texttt{test-id8}, \texttt{test-id10} \\
      \hline
      \textbf{Data Integrity} 
        (SRS~\ref{req-data-integrity}) 
      & \texttt{test-id10} \\
      \hline
      \textbf{Usability} 
        (SRS~\ref{req-usability}) 
      & \texttt{test-id11} \\
      \hline
    \end{tabular}
  \caption{Traceability Between Test Cases and Requirements (Links to SRS Sections)}
\end{table}

\clearpage

\section{Nondynamic Testing Plan}
In addition to the dynamic tests described above, the following nondynamic testing techniques will be used:

\begin{itemize}
    \item \textbf{Static Code Analysis}: We will run linters and static analyzers (e.g., \textit{ESLint} for JavaScript, \textit{Pylint} for Python) to detect potential errors, stylistic issues, and security vulnerabilities before runtime.
    \item \textbf{Code Reviews and Inspections}: Each major feature will undergo a formal peer review, with at least two team members examining the code against project coding standards and the design described in the MIS.
    \item \textbf{Document Reviews}: Key project artifacts (SRS, design documents, user manuals) will be reviewed for consistency and correctness. This includes verifying that naming conventions, references, and requirement statements align with each other and the code base.
    \item \textbf{Walkthroughs and Technical Demos}: At milestone meetings, we will conduct walkthroughs of new or complex features to validate design decisions and ensure maintainability.
\end{itemize}

These nondynamic activities supplement the dynamic tests by identifying defects early, ensuring code quality, and maintaining documentation accuracy.

\section{Unit Test Description}

\subsection{Reference to MIS and Unit Testing Philosophy}
After the MIS (detailed design document) is finalized, we will reference each software module (and its Access Programs) in the MIS. Our philosophy is to:

\begin{itemize}
    \item Write unit tests for each major function or class constructor, covering both normal operation and edge cases (boundary values, invalid inputs, etc.).
    \item Focus on the most critical or complex modules first (e.g., financial calculations, authorization checks).
    \item Maintain a \texttt{test/} directory in the repository, with test files named after the modules they verify, and meaningful test method names.
    \item Automatically run these tests in a continuous integration (CI) pipeline.
\end{itemize}

\subsection{Unit Testing Scope}
\begin{itemize}
    \item \textbf{In Scope}: Core modules owned by the project team, including reimbursement processing, budget management, user authentication, and notification handling.
    \item \textbf{Out of Scope}: Third-party libraries (e.g., payment gateways) if those are externally maintained. We will rely on their documentation and standard library tests, but we will write integration tests to confirm that calls to these libraries work as expected.
\end{itemize}

\subsection{Tests for Functional Requirements at the Unit Level}
Once the MIS is complete, each module’s responsibilities (as per the design) will be mapped to corresponding unit tests. 

\subsubsection{Module 1 (Example)}
\textit{(Reference MIS Section 3.2.1 for Module 1 design)}

\begin{enumerate}
    \item \textbf{test-module1-id1} \\
        \textbf{Type}: Functional, Dynamic, Automatic \\
        \textbf{Initial State}: An instance of Module 1 with default configuration. \\
        \textbf{Input}: A typical valid input object (e.g., \texttt{Expense \{desc: "Pens", amount: 15\}}). \\
        \textbf{Output}: The module should return \texttt{true}, indicating successful processing. \\
        \textbf{Test Case Derivation}: Based on normal operation scenario from the MIS design. \\
        \textbf{How Test Will Be Performed}: 
        \begin{itemize}
            \item Instantiate the module with default settings.
            \item Invoke the method with the valid input.
            \item Assert that the result is \texttt{true}.
        \end{itemize}
    \item \textbf{test-module1-id2} \\
        \textbf{Type}: Functional, Dynamic, Automatic \\
        \textbf{Initial State}: Same as above. \\
        \textbf{Input}: An invalid expense object (e.g., negative amount). \\
        \textbf{Output}: The module should raise an error or return \texttt{false}. \\
        \textbf{Test Case Derivation}: Based on edge-case handling in MIS. \\
        \textbf{How Test Will Be Performed}:
        \begin{itemize}
            \item Instantiate the module.
            \item Pass an invalid input object (negative or zero \texttt{amount}).
            \item Assert that an error is thrown or that the return is \texttt{false}.
        \end{itemize}
\end{enumerate}

\subsubsection{Module 2 (Example)}
\textit{(Reference MIS Section 3.2.2 for Module 2 design)}

\begin{enumerate}
    \item \textbf{test-module2-id1} \\
        \textbf{Type}: Functional, Dynamic, Automatic \\
        \textbf{Initial State}: Module 2 is instantiated with user authentication mock. \\
        \textbf{Input}: Valid user token with certain role. \\
        \textbf{Output}: The method returns \texttt{true} indicating the user is allowed to approve. \\
        \textbf{How Test Will Be Performed}:
        \begin{itemize}
            \item Create a mock user object with \texttt{role="admin"}.
            \item Call the \texttt{hasApprovalAccess()} method.
            \item Assert that the method returns \texttt{true}.
        \end{itemize}
    \item \textbf{test-module2-id2} \\
        \textbf{Type}: Functional, Dynamic, Automatic \\
        \textbf{Initial State}: Same as above. \\
        \textbf{Input}: Invalid user role (e.g., \texttt{role="employee"}). \\
        \textbf{Output}: The method returns \texttt{false} or denies access. \\
        \textbf{How Test Will Be Performed}:
        \begin{itemize}
            \item Create a mock user object with \texttt{role="employee"}.
            \item Call the \texttt{hasApprovalAccess()} method.
            \item Assert that the method returns \texttt{false}.
        \end{itemize}
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements at the Unit Level}
If a particular module has performance-critical functions (e.g., complex financial calculations), we will write micro-benchmark tests to measure execution time or memory usage. For example:

\subsubsection{Module X (Performance Example)}
\begin{enumerate}
    \item \textbf{test-moduleX-perf1} \\
        \textbf{Type}: Performance, Dynamic, Automatic \\
        \textbf{Initial State}: Module X loaded with typical data set. \\
        \textbf{Input/Condition}: 10,000 calculation calls in quick succession. \\
        \textbf{Output/Result}: The average execution time should not exceed 10ms per call on standard hardware. \\
        \textbf{How Test Will Be Performed}:
        \begin{itemize}
            \item Run a loop calling the function 10,000 times.
            \item Log the total execution time.
            \item Calculate the average time and assert it is under 10ms.
        \end{itemize}
\end{enumerate}

\clearpage
\section{Appendix A: Usability Survey Checklist (Sample)}
\begin{itemize}
    \item \textbf{Task Completion Time}: Record how long it takes each user to complete a reimbursement submission (goal: $<$ 1 minute).
    \item \textbf{UI Clarity}: Rate from 1 (confusing) to 5 (very clear).
    \item \textbf{Navigation}: Rate from 1 (hard to find pages) to 5 (easy to navigate).
    \item \textbf{Overall Satisfaction}: Rate from 1 (very dissatisfied) to 5 (very satisfied).
\end{itemize}
Participants can also provide open-ended comments on what improvements they'd like to see.


\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

N/A

\subsection{Symbolic Parameters}

N/A

\subsection{Usability Survey Questions?}

N/A

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? \\
  \textbf{Sufyan:} What went well for me this assignment was getting github actions to build the pdf on commit. I also enjoyed learning about all the tooling we could use for the project to ensure code quality, test coverage and security. \\
  \textbf{Taaha} What went well while writing this deliverable was the amount of support I had when asking teammates questions. Was extremely helpful whenever I came across an issue I was unsure of. \\
  \textbf{Omar:} What went well was the clarity of this document, we understand our project so we were able to devise this document with little to no questions asked to the TA. \\
  \textbf{Housam:} One aspect that went well for me was refining the plan structure to align with the project’s goals and requirements. Working on the document also helped solidify my understanding of the V\&V process and how it integrates into the larger project goals. \\
  \textbf{Rachid:} What went well for me this deliverable was that we discussed how we were going to work on this very early and this allowed me to spend more time thinking and working on this deliverable

  \item What pain points did you experience during this deliverable, and how
    did you resolve them? \\
  \textbf{Sufyan:} The pain points I experienced were getting the github actions to work properly. I had to do a lot of research and trial and error to get it to work. I also had some issues with the formatting of the document. I resolved these issues by asking for help from my team members. \\
  \textbf{Taaha} The pain points I experienced were mostly related to formatting the document correctly. In order to resolve this, I got help from teammates and looked online. \\
  \textbf{Omar:} I got really sick during this deliverable which slowed down my progress a lot, in addition, it was harder to manage this deliverable since I had a bunch of other deliverables due near the same time. \\
  \textbf{Housam:} The main challenge I faced was balancing detail with conciseness, as I wanted to cover each point in the plan without making it overly lengthy. At times, I felt I was going into too much detail, so I reviewed my work multiple times to find a clear and concise way to present information. Discussing with teammates helped refine certain sections and focus on essential content. \\
  \textbf{Rachid:} This deliverable was extremely clear and as a result, I experienced no pain points.

  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member. 


    
    \textbf{What knowledge and skills will the team collectively need to acquire to successfully complete the verification and validation of your project?}
    
    Key skills needed for V\&V include:
    
    \begin{itemize}
        \item \textbf{Dynamic Testing:} Using Jest and Cypress for unit and integration testing.
        \item \textbf{Static Analysis:} Using ESLint and SonarQube to identify code issues.
        \item \textbf{CI/CD Setup:} Configuring GitHub Actions to automate testing and deployment.
        \item \textbf{Security Testing:} Implementing npm audit for vulnerability scanning.
        \item \textbf{Usability Testing:} Conducting accessibility and usability tests using Lighthouse.
    \end{itemize}

  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
  
  \begin{itemize}
    \item \textbf{Dynamic Testing:} Team tutorials on Jest/Cypress and applying tests in the project, led by Housam.
    \item \textbf{Static Analysis:} Reviewing ESLint/SonarQube documentation and integrating tools during development, led by Omar.
    \item \textbf{CI/CD Setup:} Using GitHub documentation and incremental setup of GitHub Actions, led by Sufyan.
    \item \textbf{Security Testing:} Researching secure coding and using npm audit, led by Taaha.
    \item \textbf{Usability Testing:} Exploring Lighthouse and gathering user feedback, coordinated by Rachid.
\end{itemize}

\end{enumerate}

\end{document}